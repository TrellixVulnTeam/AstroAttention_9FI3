{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Epoch Based Attention**\n",
    "Outline to play around with. Move sections to individual files as planned and required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "#PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "#Torchvision\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "#Other\n",
    "import PIL\n",
    "from torchsummary import summary\n",
    "#from FRDEEP import FRDEEPF\n",
    "from MiraBest import MiraBest_full\n",
    "from models_new import *\n",
    "from models.networks_other import init_weights\n",
    "from skimage.transform import resize\n",
    "\n",
    "#Special Plot Functions\n",
    "import matplotlib.cm as cm\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions / Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will help remove Attention Images as required throughout training\n",
    "class HookBasedFeatureExtractor(nn.Module):\n",
    "    def __init__(self, submodule, layername, upscale=False):\n",
    "        super(HookBasedFeatureExtractor, self).__init__()\n",
    "\n",
    "        self.submodule = submodule\n",
    "        self.submodule.eval()\n",
    "        self.layername = layername\n",
    "        self.outputs_size = None\n",
    "        self.outputs = None\n",
    "        self.inputs = None\n",
    "        self.inputs_size = None\n",
    "        self.upscale = upscale\n",
    "\n",
    "    def get_input_array(self, m, i, o):\n",
    "        if isinstance(i, tuple):\n",
    "            self.inputs = [i[index].data.clone() for index in range(len(i))]\n",
    "            self.inputs_size = [input.size() for input in self.inputs]\n",
    "        else:\n",
    "            self.inputs = i.data.clone()\n",
    "            self.inputs_size = self.input.size()\n",
    "        #print('Input Array Size: ', self.inputs_size)\n",
    "\n",
    "    def get_output_array(self, m, i, o):\n",
    "        if isinstance(o, tuple):\n",
    "            self.outputs = [o[index].data.clone() for index in range(len(o))]\n",
    "            self.outputs_size = [output.size() for output in self.outputs]\n",
    "        else:\n",
    "            self.outputs = o.data.clone()\n",
    "            self.outputs_size = self.outputs.size()\n",
    "        #print('Output Array Size: ', self.outputs_size)\n",
    "\n",
    "    def rescale_output_array(self, newsize):\n",
    "        us = nn.Upsample(size=newsize[2:], mode='bilinear')\n",
    "        if isinstance(self.outputs, list):\n",
    "            for index in range(len(self.outputs)): self.outputs[index] = us(self.outputs[index]).data()\n",
    "        else:\n",
    "            self.outputs = us(self.outputs).data()\n",
    "\n",
    "    def forward(self, x):\n",
    "        target_layer = self.submodule._modules.get(self.layername)\n",
    "\n",
    "        # Collect the output tensor\n",
    "        h_inp = target_layer.register_forward_hook(self.get_input_array)\n",
    "        h_out = target_layer.register_forward_hook(self.get_output_array)\n",
    "        self.submodule(x)\n",
    "        h_inp.remove()\n",
    "        h_out.remove()\n",
    "\n",
    "        # Rescale the feature-map if it's required\n",
    "        if self.upscale: self.rescale_output_array(x.size())\n",
    "\n",
    "        return self.inputs, self.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotNNFilterOverlay(input_im, units, figure_id, interp='bilinear', colormap=cm.viridis, colormap_lim=None, title='', alpha=0.8):\n",
    "    \n",
    "    try:\n",
    "        filters = units.shape[2]\n",
    "    except:\n",
    "        filters = 1\n",
    "    fig = plt.figure(figure_id, figsize=(10,10))\n",
    "    fig.clf()\n",
    "    \n",
    "    for i in range(filters):\n",
    "        plt.imshow(input_im[:,:,0], interpolation=interp, cmap='gray')\n",
    "        plt.imshow(units[:,:,i], interpolation=interp, cmap=colormap, alpha=alpha)\n",
    "        plt.axis('off')\n",
    "        plt.colorbar()\n",
    "        plt.title(title, fontsize='small')\n",
    "        if colormap_lim:\n",
    "            plt.clim(colormap_lim[0],colormap_lim[1])\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My cleaned func to be called at will... Theoretically.\n",
    "def attentions_func(batch_of_images,net):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        batch_of_images: Images with type==torch.tensor, of dimension (-1,1,150,150)\n",
    "        net: model being loaded in.\n",
    "    Calls on: HookBasedFeatureExtractor to call out designed attention maps.\n",
    "    Output: Upscaled Attention Maps, Attention Maps\n",
    "    \n",
    "    \"\"\"\n",
    "    images = batch_of_images\n",
    "    for iteration in range(batch_of_images.shape[0]):\n",
    "        for i in [1,2]:\n",
    "            feature_extractor = HookBasedFeatureExtractor(net, f'compatibility_score{i}', upscale=False)\n",
    "            imap, fmap = feature_extractor.forward(images.to(device)) # imap are the inputs from the class and fmap are the outputs. Both are \n",
    "            #print(imap[0].cpu().numpy().shape)\n",
    "            if not fmap: #Will pass if fmap is none or empty etc. \n",
    "                continue #(ie. Skips iteration if compatibility_score{i} does not compute with the network.)\n",
    "\n",
    "            #print('fmap[0]: ',fmap[0].numpy().shape)\n",
    "            #print('fmap[1]: ',fmap[1].numpy().shape,'This is the attention map! (shape[1]=1)')\n",
    "            attention = fmap[1].cpu().numpy()[iteration,0]\n",
    "\n",
    "            attention = np.expand_dims(resize(attention, (150, 150), mode='edge', preserve_range=True),axis=2)\n",
    "            try:\n",
    "                attentions = np.append(attentions,attention, axis=2)\n",
    "                AMap_originals= np.append(AMap_originals,np.expand_dims(fmap[1].cpu().numpy()[iteration,0],axis=2),axis=2)\n",
    "            except:\n",
    "                attentions = attention\n",
    "                AMap_originals = np.expand_dims(fmap[1].cpu().numpy()[iteration,0],axis=2)\n",
    "    return attentions , AMap_originals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Read in data and prep it (define transformations / batches and download)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([transforms.RandomRotation([0,360],resample=PIL.Image.BILINEAR),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.5],[0.5])\n",
    "                                     ])\n",
    "out_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.5],[0.5])\n",
    "                                    ])\n",
    "train_transform = transforms.Compose([transforms.RandomRotation([0,360],resample=PIL.Image.BILINEAR),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.5],[0.5])\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define sampling function used to randomly split training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sampler(trainset, valid_size = 0.):\n",
    "    # obtain training indices that will be used for validation\n",
    "    num_train = len(trainset)\n",
    "    indices = list(range(num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    \n",
    "    return (train_sampler, valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "1099 samples in the traindata: traindata.shape: 9,torch.Size([1, 150, 150])\n",
      "157 samples in the testdata: testdata.shape: 9,torch.Size([1, 150, 150])\n",
      "157 samples in the outdata: outdata.shape: 9,torch.Size([1, 150, 150])\n"
     ]
    }
   ],
   "source": [
    "#traindata = datasets.MNIST('', train = True, download = True, transform=transform)\n",
    "#testdata = datasets.MNIST('', train = False, download = True, transform = transform)\n",
    "\n",
    "traindata = MiraBest_full(root='./FIRST_data', train=True, download=True, transform=train_transform)\n",
    "testdata = MiraBest_full(root='./FIRST_data', train=False, download=True, transform=test_transform)\n",
    "outdata = MiraBest_full(root='./FIRST_data', train=False, download=True, transform=out_transform)\n",
    "\n",
    "\n",
    "counter=0\n",
    "for data in traindata:\n",
    "    counter+=1\n",
    "print(f'{counter} samples in the traindata: traindata.shape: {data[1]},{data[0].shape}')\n",
    "\n",
    "counter=0\n",
    "for data in testdata:\n",
    "    counter+=1\n",
    "print(f'{counter} samples in the testdata: testdata.shape: {data[1]},{data[0].shape}')\n",
    "\n",
    "counter=0\n",
    "for data in outdata:\n",
    "    counter+=1\n",
    "print(f'{counter} samples in the outdata: outdata.shape: {data[1]},{data[0].shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch, shuffle / sample the test, validation and training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "valid_size = 0.2\n",
    "train_sampler,valid_sampler = Sampler(traindata, valid_size=valid_size)\n",
    "\n",
    "outset = torch.utils.data.DataLoader(outdata, batch_size=batch_size)\n",
    "trainset = torch.utils.data.DataLoader(traindata, batch_size=batch_size, sampler=train_sampler)\n",
    "validset = torch.utils.data.DataLoader(traindata, batch_size=batch_size, sampler=valid_sampler)\n",
    "testset = torch.utils.data.DataLoader(testdata, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Helpful dimensionality visualisation on input values.\n",
    "for batch in outset:\n",
    "    plt.imshow(batch[0][8][0],cmap='gray')\n",
    "    break\n",
    "#testdata = torch.randn(1,1,150,150)\n",
    "#plt.imshow(testdata[0][0].view(150,150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network\n",
    "Defining our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# This is the one we are acctually using. We call it in sononet_grid_attention.py as AttentionBlock2D. Input Changes:\n",
    "# 'dimension = 2' and 'sub_sample_factor = (1,1)'\n",
    "class _GridAttentionBlock2D_TORR(nn.Module): #Cleaned up from _GridAttentionBlockND_TORR(...)\n",
    "    def __init__(self, in_channels, gating_channels, inter_channels=None, dimension=2, mode='concatenation_softmax',\n",
    "                 sub_sample_factor=(1,1), bn_layer=True, use_W=False, use_phi=True, use_theta=True, use_psi=True, nonlinearity1='relu'):\n",
    "        super(_GridAttentionBlock2D_TORR, self).__init__()\n",
    "\n",
    "        assert dimension in [2] #for 3 dimensional functionality, use original implementation of functions.\n",
    "        assert mode in ['concatenation_softmax'] #Removed all other options for legibility.\n",
    "\n",
    "        # Default parameter set\n",
    "        self.mode = mode\n",
    "        self.dimension = dimension\n",
    "        self.sub_sample_factor = sub_sample_factor if isinstance(sub_sample_factor, tuple) else tuple([sub_sample_factor])*dimension\n",
    "        self.sub_sample_kernel_size = self.sub_sample_factor\n",
    "\n",
    "        # Number of channels\n",
    "        self.in_channels = in_channels\n",
    "        self.gating_channels = gating_channels\n",
    "        self.inter_channels = inter_channels\n",
    "\n",
    "        if self.inter_channels is None: # This is the value we use.\n",
    "            self.inter_channels = in_channels // 2 #Either half of in_channels (in_channels > 1) or = 1 (in_channels = 1).\n",
    "            if self.inter_channels == 0:\n",
    "                self.inter_channels = 1 #We go down to one channel because of this!\n",
    "\n",
    "        if dimension == 2:\n",
    "            conv_nd = nn.Conv2d\n",
    "            bn = nn.BatchNorm2d\n",
    "            self.upsample_mode = 'bilinear'\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "\n",
    "        # initialise id functions\n",
    "        # Theta^T * x_ij + Phi^T * gating_signal + bias\n",
    "        self.W = lambda x: x        # These are essentially base functions for if any of the conditions (below) aren't met,\n",
    "        self.theta = lambda x: x    # in which case each of these methods returns the data without any alterations / augmentations. ie. x -> x\n",
    "        self.psi = lambda x: x\n",
    "        self.phi = lambda x: x\n",
    "        self.nl1 = lambda x: x\n",
    "        \n",
    "        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels, kernel_size=(1,1), stride=(1,1), padding=0, bias=False)\n",
    "        self.phi = conv_nd(in_channels=self.gating_channels, out_channels=self.inter_channels, kernel_size=(1,1), stride=(1,1), padding=0, bias=False)\n",
    "        self.psi = conv_nd(in_channels=self.inter_channels, out_channels=1, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        self.nl1 = lambda x: F.relu(x, inplace=True)\n",
    "\n",
    "        ### Initialise weights using their package (see imports) ###\n",
    "        for m in self.children():\n",
    "            init_weights(m, init_type='kaiming')\n",
    "        #if use_psi and self.mode == 'concatenation_softmax':\n",
    "        nn.init.constant(self.psi.bias.data, 10.0) # Initialises the tensor self.psi.bias.data with values of 10.0 (Because bias=True in initialisation)\n",
    "\n",
    "\n",
    "    def forward(self, x, g):\n",
    "        # As we assert that mode must contain concatenation, this holds for all passes where we pass the initial assertion\n",
    "        #(this was a seperate method, called _concatenation - see sononet_grid_attention.py).\n",
    "        '''\n",
    "        :param x: (b, c, t, h, w),\n",
    "        ie. (batch dim, channel dim, thickness, height, width), in our case we omit thickness as we are working with 2D data.\n",
    "        :param g: (b, g_d)\n",
    "        :return:\n",
    "        '''\n",
    "        input_size = x.size()\n",
    "        batch_size = input_size[0]\n",
    "        assert batch_size == g.size(0)\n",
    "        \n",
    "        # Compute compatibility score: psi_f\n",
    "        #print(x.size())\n",
    "        theta_x = self.theta(x)\n",
    "        theta_x_size = theta_x.size()\n",
    "        \n",
    "        # nl(theta.x + phi.g + bias) -> f = (b, i_c, t, h/s2, w/s3)\n",
    "        phi_g = F.upsample(self.phi(g), size=theta_x_size[2:], mode=self.upsample_mode)\n",
    "        f = theta_x + phi_g\n",
    "        f = self.nl1(f)\n",
    "\n",
    "        psi_f = self.psi(f)\n",
    "\n",
    "        # Calculate Attention map (sigm_psi_f) and weighted output (W_y)\n",
    "        # This block was conditional with: 'self.mode == concatenation_softmax'. Other options are listed in grid_attention_layer.py\n",
    "        # normalisation & scale to x.size()[2:]\n",
    "        # psi^T . f -> (b, 1, t/s1, h/s2, w/s3)\n",
    "        sigm_psi_f = F.softmax(psi_f.view(batch_size, 1, -1), dim=2)\n",
    "        sigm_psi_f = sigm_psi_f.view(batch_size, 1, *theta_x_size[2:])\n",
    "\n",
    "        # sigm_psi_f is attention map! upsample the attentions and multiply\n",
    "        sigm_psi_f = F.upsample(sigm_psi_f, size=input_size[2:], mode=self.upsample_mode) ### mode = bilinear in 2D, ipnut_size is the input WxH of the data.\n",
    "        y = sigm_psi_f.expand_as(x) * x\n",
    "        W_y = self.W(y) #As W = False, W_y = y\n",
    "\n",
    "        return W_y, sigm_psi_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class playgroundv1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(playgroundv1,self).__init__()\n",
    "        filters = [6,16,16,32,64,128]\n",
    "        #ksizes = [11,5,5,3,3,3] # Must all be odd for calculation of padding.\n",
    "        ksizes = [3,3,3,3,3,3] # Must all be odd for calculation of padding.        \n",
    "        self.filters = filters\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1,         out_channels=filters[0],\n",
    "                               kernel_size=ksizes[0],padding=ksizes[0]//2,stride=1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=filters[0],out_channels=filters[1],\n",
    "                               kernel_size=ksizes[1],padding=ksizes[1]//2,stride=1)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=filters[1],out_channels=filters[2],\n",
    "                               kernel_size=ksizes[2],padding=ksizes[2]//2,stride=1)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(in_channels=filters[2],out_channels=filters[3],\n",
    "                               kernel_size=ksizes[3],padding=ksizes[3]//2,stride=1)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(in_channels=filters[3],out_channels=filters[4],\n",
    "                               kernel_size=ksizes[4],padding=ksizes[4]//2,stride=1)\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(in_channels=filters[4],out_channels=filters[5],\n",
    "                               kernel_size=ksizes[5],padding=ksizes[5]//2,stride=1)\n",
    "        \n",
    "        self.mpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.mpool2 = nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        self.mpool3 = nn.MaxPool2d(kernel_size=5, stride=5)\n",
    "        \n",
    "        self.bnorm1 = nn.BatchNorm2d(filters[0])\n",
    "        self.bnorm2 = nn.BatchNorm2d(filters[1])\n",
    "        self.bnorm3 = nn.BatchNorm2d(filters[2])\n",
    "        self.bnorm4 = nn.BatchNorm2d(filters[3])\n",
    "        self.bnorm5 = nn.BatchNorm2d(filters[4])\n",
    "        self.bnorm6 = nn.BatchNorm2d(filters[5])\n",
    "        \n",
    "        self.flatten = nn.Flatten(1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(16*5*5,256) #channel_size * width * height\n",
    "        self.fc2 = nn.Linear(256,256)\n",
    "        self.fc3 = nn.Linear(256,2)\n",
    "        \n",
    "        self.dropout = nn.Dropout()\n",
    "    \n",
    "        ##########\n",
    "        # These are effectively the same calls as in sononet_grid_attention.py due to us redefining the standard selections in _GridAttentionBlock2D(..)        \n",
    "        self.compatibility_score1 = _GridAttentionBlock2D_TORR(in_channels=filters[3] , gating_channels=filters[5], inter_channels=filters[2], use_W=False) ### Why did they choose use_W = False ?\n",
    "        self.compatibility_score2 = _GridAttentionBlock2D_TORR(in_channels=filters[4], gating_channels=filters[5], inter_channels=filters[2], use_W=False)\n",
    "        \n",
    "        # Primary Aggregation selection: (simplest format): ie. >>> if 'concat':\n",
    "        self.classifier = nn.Linear(filters[3]+filters[4]+filters[5], 2) ### NUMBER OF CLASSES TO BE CLASSIFIED INTO IS HERE! CARE !!!\n",
    "        self.aggregate = self.aggregation_concat\n",
    "        \n",
    "    def aggregation_concat(self, *attended_maps):\n",
    "        #print('TORCH CATINATION GRAPHS: ',torch.cat(attended_maps,dim=1).shape)\n",
    "        return self.classifier(torch.cat(attended_maps, dim=1))\n",
    "        ##########\n",
    "    \n",
    "    # Activation function\n",
    "    @staticmethod\n",
    "    def apply_argmax_softmax(pred):\n",
    "        log_p = F.softmax(pred, dim=1)\n",
    "        return log_p\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        conv1  = F.relu(self.conv1(inputs))\n",
    "        bnorm1 = self.bnorm1(conv1)\n",
    "        mpool1 = self.mpool1(bnorm1)\n",
    "        \n",
    "        conv2  = F.relu(self.conv2(mpool1))\n",
    "        bnorm2 = self.bnorm2(conv2)\n",
    "        mpool2 = self.mpool2(bnorm2)\n",
    "        \n",
    "        ###\n",
    "        # Arbitrarily added to extend size of network (goal: hopefully better classification - seemed to not be 'learning')\n",
    "        conv3  = F.relu(self.conv3(mpool2))\n",
    "        bnorm3 = self.bnorm3(conv3)\n",
    "        #mpool2 = bnorm2\n",
    "        \n",
    "        conv3  = F.relu(self.conv3(bnorm3))\n",
    "        bnorm3 = self.bnorm3(conv3)\n",
    "        \n",
    "        conv3  = F.relu(self.conv3(bnorm3))\n",
    "        bnorm3 = self.bnorm3(conv3)\n",
    "        mpool3 = bnorm2\n",
    "        ###\n",
    "        \n",
    "        conv4  = F.relu(self.conv4(mpool3))\n",
    "        bnorm4 = self.bnorm4(conv4)\n",
    "        \n",
    "        conv5  = F.relu(self.conv5(bnorm4))\n",
    "        bnorm5 = self.bnorm5(conv5)\n",
    "        \n",
    "        conv6  = F.relu(self.conv6(bnorm5))\n",
    "        bnorm6 = self.bnorm6(conv6)\n",
    "        mpool3 = self.mpool3(bnorm6)\n",
    "        \n",
    "        #flatten = self.flatten(mpool3)\n",
    "        #fc1     = F.relu(self.fc1(flatten))\n",
    "        #do      = self.dropout(fc1)\n",
    "        #fc2     = F.relu(self.fc2(do))\n",
    "        #do      = self.dropout(fc2)\n",
    "        #fc2     = F.relu(self.fc2(do))\n",
    "        #do      = self.dropout(fc2)\n",
    "        #fc3     = F.relu(self.fc3(do))\n",
    "        \n",
    "        # Applied Attention , Attention map\n",
    "        #We use conv5 as the global attention as this is the most advanced stage (upsampled to conv3/conv4 to make it work).\n",
    "        #(inputs are before maxpool layer in sononet_grid_attention.py)\n",
    "        #print(conv3.size(),conv4.size(),conv5.size())\n",
    "        #print('4, 5 and 6 shapes: \\n', conv4.shape,conv5.shape,conv6.shape)\n",
    "        attendedConv3 , atten3 = self.compatibility_score1(conv4, conv6)\n",
    "        attendedConv4 , atten4 = self.compatibility_score2(conv5, conv6)\n",
    "        \n",
    "        filters = self.filters\n",
    "        batch_size = inputs.shape[0]\n",
    "        \n",
    "        # Aggregation\n",
    "        pooled = F.adaptive_avg_pool2d(conv6,(1,1)).view(batch_size,-1)\n",
    "        g1 = torch.sum(attendedConv3.view(batch_size, filters[3], -1), dim=-1)\n",
    "        g2 = torch.sum(attendedConv4.view(batch_size, filters[4], -1), dim=-1)\n",
    "        \n",
    "        #print('TOTAL SIZE OF LINAR FUNCTION SHOULD BE: ',filters[4]+2*filters[5])\n",
    "        #print('G1 , G2 and POOLED shape: ',g1.shape,g2.shape,pooled.shape)\n",
    "        out = self.aggregate(g1,g2,pooled)\n",
    "        \n",
    "        \n",
    "        #return F.log_softmax(out,dim=1)\n",
    "        return F.softmax(out,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will be putting our network and data on >> cuda <<\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "print(f\"We will be putting our network and data on >> {device} <<\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select network from any of the listed options, move it to the gpu if available and then print the summary of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/scratch/mbowles/Attention-MSc/PYTHON/models/networks_other.py:42: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n",
      "/raid/scratch/mbowles/Attention-MSc/PYTHON/models_new/playground_v2.py:257: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  nn.init.constant(self.psi.bias.data, 10.0) # Initialises the tensor self.psi.bias.data with values of 10.0 (Because bias=True in initialisation)\n",
      "/raid/scratch/mbowles/Attention-MSc/PYTHON/models/networks_other.py:46: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(m.weight.data, 1.0, 0.02)\n",
      "/raid/scratch/mbowles/Attention-MSc/PYTHON/models/networks_other.py:47: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(m.bias.data, 0.0)\n"
     ]
    }
   ],
   "source": [
    "from models_new import *\n",
    "\n",
    "#net = playground(); net_name = 'playground'\n",
    "#net = playgroundv1(); net_name = 'playgroundv1'\n",
    "#net = playgroundv2(aggregation_mode='concat'); net_name = 'playgroundv2_concat'\n",
    "#net = playgroundv2(aggregation_mode='mean'); net_name = 'playgroundv2_mean'\n",
    "#net = playgroundv2(aggregation_mode='deep_sup'); net_name = 'playgroundv2_deep_sup'\n",
    "net = playgroundv2(aggregation_mode='ft'); net_name = 'playgroundv2_ft'\n",
    "#net = transfer_original(); net_name = 'transfer_original'\n",
    "#net = AGSononet(); net_name = 'AGSononet'\n",
    "#net = AGTransfer(); net_name = 'AGTransfer'\n",
    "\n",
    "# Put network on device defined previously.\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 6, 150, 150]              60\n",
      "       BatchNorm2d-2          [-1, 6, 150, 150]              12\n",
      "         MaxPool2d-3            [-1, 6, 75, 75]               0\n",
      "            Conv2d-4           [-1, 16, 75, 75]             880\n",
      "       BatchNorm2d-5           [-1, 16, 75, 75]              32\n",
      "         MaxPool2d-6           [-1, 16, 25, 25]               0\n",
      "            Conv2d-7           [-1, 16, 25, 25]           2,320\n",
      "       BatchNorm2d-8           [-1, 16, 25, 25]              32\n",
      "            Conv2d-9           [-1, 16, 25, 25]           2,320\n",
      "      BatchNorm2d-10           [-1, 16, 25, 25]              32\n",
      "           Conv2d-11           [-1, 16, 25, 25]           2,320\n",
      "      BatchNorm2d-12           [-1, 16, 25, 25]              32\n",
      "           Conv2d-13           [-1, 32, 75, 75]           4,640\n",
      "      BatchNorm2d-14           [-1, 32, 75, 75]              64\n",
      "           Conv2d-15           [-1, 64, 75, 75]          18,496\n",
      "      BatchNorm2d-16           [-1, 64, 75, 75]             128\n",
      "           Conv2d-17          [-1, 128, 75, 75]          73,856\n",
      "      BatchNorm2d-18          [-1, 128, 75, 75]             256\n",
      "        MaxPool2d-19          [-1, 128, 15, 15]               0\n",
      "           Conv2d-20           [-1, 16, 75, 75]             512\n",
      "           Conv2d-21           [-1, 16, 75, 75]           2,048\n",
      "           Conv2d-22            [-1, 1, 75, 75]              17\n",
      "_GridAttentionBlock2D_TORR-23  [[-1, 32, 75, 75], [-1, 1, 75, 75]]               0\n",
      "           Conv2d-24           [-1, 16, 75, 75]           1,024\n",
      "           Conv2d-25           [-1, 16, 75, 75]           2,048\n",
      "           Conv2d-26            [-1, 1, 75, 75]              17\n",
      "_GridAttentionBlock2D_TORR-27  [[-1, 64, 75, 75], [-1, 1, 75, 75]]               0\n",
      "           Linear-28                    [-1, 2]              66\n",
      "           Linear-29                    [-1, 2]             130\n",
      "           Linear-30                    [-1, 2]             258\n",
      "           Linear-31                    [-1, 2]              14\n",
      "================================================================\n",
      "Total params: 111,614\n",
      "Trainable params: 111,614\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.09\n",
      "Forward/backward pass size (MB): 23147.78\n",
      "Params size (MB): 0.43\n",
      "Estimated Total Size (MB): 23148.29\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda-python-3.6/lib/python3.6/site-packages/torch/nn/functional.py:2390: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/usr/local/anaconda-python-3.6/lib/python3.6/site-packages/torch/nn/functional.py:2479: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    }
   ],
   "source": [
    "summary(net, (1,150,150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Defining optimizer, loss_function, epoch, training etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch = 360\n",
    "validation_epoch = 360 #No. of transformations made on validation set to create total validation set.\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "learning_rate = 1.0e-2 #Adagrad: 1.0e-(5,6,7) didnt work #Adam: 1.0e-4 didnt work\n",
    "#optimizer = optim.Adagrad(net.parameters(), lr=learning_rate) #Best lr: 1.0e-3 (so far)\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, nesterov=True); varying_optimizer_sononet = True\n",
    "#optimizer = optim.Adadelta(net.parameters()) ### lr = 1.0 standard <-- https://pytorch.org/docs/stable/optim.html\n",
    "#optimizer = optim.Adam(net.parameters(),lr=learning_rate) # lr = 0.001 standard #Best lr: 1.0e-5 (so far - doesnt go past 0.5 for 360 epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Saved Under TrainedNetworks/playgroundv2_ft-0310-MiraBestSGDVariedLR-0.01_360Epochs.pt\n",
      "Directory to save model already exists (True).\n"
     ]
    }
   ],
   "source": [
    "date = '0311-MiraBestSGDVariedLR' #format: mmdd\n",
    "ckpt_name = f\"{net_name}-{date}-{learning_rate}_{Epoch}Epochs.pt\"\n",
    "folder_name = f\"TrainedNetworks/{date}-{net_name}\"\n",
    "print(f\"Final Model Saved Under TrainedNetworks/{ckpt_name}\")\n",
    "try:\n",
    "    os.mkdir(folder_name)\n",
    "    print(f'Directory to save model created: {os.path.isdir(folder_name)}')\n",
    "except:\n",
    "    print(f'Directory to save model already exists ({os.path.isdir(folder_name)}).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\tTraining Loss: 0.700410\t\tValidation Loss: 0.685906\n",
      "\tValidation Loss Down: \t(     inf-->0.685906) ... Updating saved model.\n",
      "Epoch:  1\tTraining Loss: 0.696346\t\tValidation Loss: 0.695361\n",
      "Epoch:  2\tTraining Loss: 0.696582\t\tValidation Loss: 0.697009\n",
      "Epoch:  3\tTraining Loss: 0.692641\t\tValidation Loss: 0.684210\n",
      "\tValidation Loss Down: \t(0.685906-->0.684210) ... Updating saved model.\n",
      "Epoch:  4\tTraining Loss: 0.682514\t\tValidation Loss: 0.716710\n",
      "Epoch:  5\tTraining Loss: 0.681088\t\tValidation Loss: 0.678112\n",
      "\tValidation Loss Down: \t(0.684210-->0.678112) ... Updating saved model.\n",
      "Epoch:  6\tTraining Loss: 0.674320\t\tValidation Loss: 0.692840\n",
      "Epoch:  7\tTraining Loss: 0.677597\t\tValidation Loss: 0.666724\n",
      "\tValidation Loss Down: \t(0.678112-->0.666724) ... Updating saved model.\n",
      "Epoch:  8\tTraining Loss: 0.668287\t\tValidation Loss: 0.819679\n",
      "Epoch:  9\tTraining Loss: 0.666037\t\tValidation Loss: 0.730725\n",
      "Epoch: 10\tTraining Loss: 0.665246\t\tValidation Loss: 0.672225\n",
      "Epoch: 11\tTraining Loss: 0.670231\t\tValidation Loss: 0.839530\n",
      "Epoch: 12\tTraining Loss: 0.660005\t\tValidation Loss: 0.839874\n",
      "Epoch: 13\tTraining Loss: 0.666524\t\tValidation Loss: 0.672208\n",
      "Epoch: 14\tTraining Loss: 0.648734\t\tValidation Loss: 0.839874\n",
      "Epoch: 15\tTraining Loss: 0.625173\t\tValidation Loss: 0.839874\n",
      "Epoch: 16\tTraining Loss: 0.619463\t\tValidation Loss: 0.743444\n",
      "Epoch: 17\tTraining Loss: 0.641518\t\tValidation Loss: 0.839874\n",
      "Epoch: 18\tTraining Loss: 0.617828\t\tValidation Loss: 0.839874\n",
      "Epoch: 19\tTraining Loss: 0.635633\t\tValidation Loss: 0.698477\n",
      "Epoch: 20\tTraining Loss: 0.623478\t\tValidation Loss: 0.670781\n",
      "Epoch: 21\tTraining Loss: 0.617329\t\tValidation Loss: 0.614049\n",
      "\tValidation Loss Down: \t(0.666724-->0.614049) ... Updating saved model.\n",
      "Epoch: 22\tTraining Loss: 0.620418\t\tValidation Loss: 0.790467\n",
      "Epoch: 23\tTraining Loss: 0.615948\t\tValidation Loss: 0.642397\n",
      "Epoch: 24\tTraining Loss: 0.605495\t\tValidation Loss: 0.604484\n",
      "\tValidation Loss Down: \t(0.614049-->0.604484) ... Updating saved model.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_net = True\n",
    "\n",
    "if train_net:\n",
    "    # Variable selections\n",
    "    valid_loss_min = np.Inf\n",
    "    num_samples = 1 #No. of samples per dataset.\n",
    "    train_loss_plot=[]\n",
    "    valid_loss_plot=[]\n",
    "    min_v_loss_plot=[]\n",
    "    #attention_maps_temp=[]\n",
    "    #epoch_updates=[]\n",
    "    #print(f'model located on: {net.device}')\n",
    "\n",
    "    # Vary the learning rate per epoch in the same fashion as sononet paper describes in 'Training' (pg.6/12)\n",
    "\n",
    "    # Set scheduler for learning rate updates throughout training\n",
    "    if varying_optimizer_sononet:\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer,milestones=[20,100,200,300],gamma=0.1)\n",
    "    \n",
    "    for epoch_count in range(Epoch):\n",
    "\n",
    "        ### Model Training ###\n",
    "        train_loss = 0.\n",
    "        valid_loss = 0.\n",
    "        net.train() #Set network to train mode.    \n",
    "        for batch_idx , (data, labels) in enumerate(trainset): #Iterates through each batch.        \n",
    "            # Put data on CPU or GPU as available.\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Create binary labels to remove morphological subclassifications\n",
    "            binary_labels = np.zeros(labels.size(),dtype=int)\n",
    "            binary_labels = np.where(labels.cpu().numpy()<5,binary_labels,binary_labels+1)\n",
    "            binary_labels = torch.from_numpy(binary_labels).to(device)\n",
    "\n",
    "            optimizer.zero_grad() # Clear gradients of all optimised variables. ### Should I be doing this? https://pytorch.org/docs/stable/optim.html\n",
    "            outputs = net.forward(data) # Forward pass ie. predicted outputs.\n",
    "            #if type(outputs)==list and net_name[-4]=='mean':\n",
    "            #    outputs = torch.mean(torch.stack(outputs),dim=[0,1])\n",
    "            loss = loss_function(outputs, binary_labels) # Calculate batch loss.\n",
    "\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            train_loss += (loss.item()*data.size(0)) / num_samples\n",
    "\n",
    "        ### Model Validation ###\n",
    "        net.eval()\n",
    "        for epoch_valid in range(validation_epoch):\n",
    "            for batch_idx, (data, labels) in enumerate(validset):\n",
    "                data = data.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Create binary labels to remove morphological subclassifications\n",
    "                binary_labels = np.zeros(labels.size(),dtype=int)\n",
    "                binary_labels = np.where(labels.cpu().numpy()<5,binary_labels,binary_labels+1)\n",
    "                binary_labels = torch.from_numpy(binary_labels).to(device)\n",
    "                \n",
    "                outputs = net.forward(data)\n",
    "                loss = loss_function(outputs, binary_labels)\n",
    "                valid_loss += (loss.item()*data.size(0)) / num_samples\n",
    "\n",
    "        # Average losses (scaled according to validation dataset size)\n",
    "        train_loss = train_loss/(len(trainset.dataset)*(1-valid_size))\n",
    "        valid_loss = valid_loss/(len(validset.dataset)*valid_size*validation_epoch)\n",
    "\n",
    "        # Print\n",
    "        print(f\"Epoch:{epoch_count:3}\\tTraining Loss: {train_loss:8.6f}\\t\\tValidation Loss: {valid_loss:8.6f}\")\n",
    "\n",
    "        # Save model if validation loss decreased (ie. best model with least overfitting)\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print(f\"\\tValidation Loss Down: \\t({valid_loss_min:8.6f}-->{valid_loss:8.6f}) ... Updating saved model.\")\n",
    "            #torch.save(net.state_dict(), f'TrainedNetworks/{ckpt_name}')\n",
    "            ckpt_name_temp = f'{epoch_count}of{Epoch}-vloss{valid_loss:.2f}'\n",
    "            torch.save(net.state_dict(), f'{folder_name}/{epoch_count}.pt')\n",
    "            valid_loss_min = valid_loss\n",
    "\n",
    "        # Save training loss / validation loss for plotting\n",
    "        train_loss_plot.append(train_loss)\n",
    "        valid_loss_plot.append(valid_loss)\n",
    "        min_v_loss_plot.append(valid_loss_min)\n",
    "        \n",
    "        # Update learning rate using scheduler\n",
    "        if varying_optimizer_sononet:\n",
    "            scheduler.step()\n",
    "\n",
    "    print(f\"\\nFinished training.\\nMinimum Validation Loss: {valid_loss_min:8.6}\\n\")\n",
    "\n",
    "else:\n",
    "    # Load the model with the lowest loss (highest epoch save no.)\n",
    "    for epoch_temp in range(Epoch):\n",
    "        print('???')\n",
    "        PATH = f'{folder_name}/{epoch_temp}.pt'\n",
    "        if os.path.exists(PATH):\n",
    "            print(\"FOUND THE PATH !!!\")\n",
    "            epoch_max = epoch_temp\n",
    "\n",
    "    net.load_state_dict(torch.load(f'{folder_name}/{epoch_max}.pt',map_location=torch.device(device)))\n",
    "    net.eval()\n",
    "    print(f'Loaded network from: {folder_name}/{epoch_max}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_net:\n",
    "    plt.figure(figsize=(8,8))\n",
    "\n",
    "    plt.subplot(211)\n",
    "    plt.plot(train_loss_plot)\n",
    "    plt.plot(min_v_loss_plot,'g')\n",
    "    plt.title(f'{net_name} Loss (lr: {learning_rate})')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim(0,1)\n",
    "    plt.grid()\n",
    "    plt.legend(['Training loss','Minimal Validation Loss'])\n",
    "\n",
    "    plt.subplot(212)\n",
    "\n",
    "    plt.plot(train_loss_plot)\n",
    "    plt.plot(valid_loss_plot)\n",
    "    plt.plot(min_v_loss_plot,'g')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.legend(['Training loss','Validation loss','Minimal Validation Loss'])\n",
    "\n",
    "    plt.savefig(f'TrainingLosses/{ckpt_name}_Losses.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Playing with the Minimal Validation Loss to see how long I have to train for initially to get a 95% similar result.\n",
    "minimal_loss = min_v_loss_plot[-1]\n",
    "values = -np.asarray(min_v_loss_plot)/minimal_loss+1\n",
    "norm_val = -np.asarray(min_v_loss_plot)/min_v_loss_plot[0]+1\n",
    "\n",
    "print(min_v_loss_plot[90])\n",
    "print(min_v_loss_plot[180])\n",
    "print(min_v_loss_plot[-1])\n",
    "# Plots\n",
    "plt.figure(figsize=(8,12))\n",
    "\n",
    "plt.subplot(311)\n",
    "plt.plot(min_v_loss_plot)\n",
    "plt.xlim(-2,360)\n",
    "plt.ylim(1.95,2.25)\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.plot(values)\n",
    "plt.xlim(-2,360)\n",
    "plt.ylim(-.13,0)\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.plot(norm_val)\n",
    "plt.xlim(-2,360)\n",
    "plt.ylim(0,-minimal_loss/min_v_loss_plot[0]+1)\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which Images will be used for epoch plot of attentions:\n",
    "sample_number = 16\n",
    "attention_map_images = []\n",
    "attention_map_labels = []\n",
    "\n",
    "# Read in my outset data (same as test set, but with no transformations)\n",
    "images = [] #Forces try except on every first iteration.\n",
    "dataiter = iter(outset)\n",
    "for data in dataiter:\n",
    "    images_temp , labels_temp = data\n",
    "    try:\n",
    "        images , labels = torch.cat((images_temp, images)), torch.cat((labels,labels_temp))\n",
    "    except:\n",
    "        images , labels = images_temp, labels_temp\n",
    "\n",
    "for i in range(-sample_number//2,sample_number//2):\n",
    "    attention_map_images.append(images.cpu().numpy()[i]) # Used as input for hook based extractor (attentions_func)\n",
    "    attention_map_labels.append(labels.cpu().numpy()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(attention_map_images[0].shape)\n",
    "plt.imshow(attention_map_images[6][0])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def AttentionImagesByEpoch(sources,folder_name,net,epoch=1500):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sources: list of Images with type==torch.tensor, of dimension (-1,1,150,150)\n",
    "        folder_name: directory of pickled .pt parameters to load into our network.\n",
    "    dependancies:\n",
    "        attentions_func()\n",
    "        HookedBasedFeatureExtraction() (from within attention_func)\n",
    "    out:\n",
    "        attention_maps_temp: array of all attention maps according to the epoch they were generated.\n",
    "        epoch_updates: list of epoch numbers for the attention map generations.\n",
    "    \"\"\"\n",
    "    \n",
    "    attention_maps_temp=[]\n",
    "    epoch_updates=[]\n",
    "\n",
    "    # Load in models in improving order based on the folder name\n",
    "    for epoch_temp in range(epoch):\n",
    "        PATH = f'{folder_name}/{epoch_temp}.pt'\n",
    "        if os.path.exists(PATH):\n",
    "            net.load_state_dict(torch.load(PATH,map_location=torch.device(device)))\n",
    "            net.eval()\n",
    "            # Generate attention maps with attentions_func and save appropriately.\n",
    "            attentions , attentions_originals_temp = attentions_func(torch.tensor(np.asarray(attention_map_images)),net)\n",
    "            for i in range(attentions.shape[2]//2):\n",
    "                mean_attentions_temp = np.mean(attentions[:,:,i*2:i*2+1],2)\n",
    "                attention_maps_temp.append(mean_attentions_temp) # Averaged attention maps of the images selected in the cell above.\n",
    "                epoch_updates.append(epoch_temp) #List of when the validation loss / attention maps were updated.        \n",
    "\n",
    "    return attention_maps_temp , epoch_updates\n",
    "\n",
    "attention_maps_temp , epoch_updates = AttentionImagesByEpoch(attention_map_images,folder_name,net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "def attention_epoch_plot(attention_maps_temp, \n",
    "                         source_images=attention_map_images,\n",
    "                         sample_number=8,\n",
    "                         logged=True,\n",
    "                         width=3,\n",
    "                         epoch_labels=['No epoch listings provided'],\n",
    "                        ):\n",
    "    \"\"\"\n",
    "    Function purely for plotting clean grid of attention maps as they develop throughout the learning stages.\n",
    "    Args:\n",
    "        The attention map data, \n",
    "        original images of sources\n",
    "        number of unique sources, \n",
    "        if you want your image logged,\n",
    "        number of output attentions desired (sampled evenly accross available space)\n",
    "        epoch labels of when the images were extracted\n",
    "    Out:\n",
    "        plt of images concatenated in correct fashion\n",
    "    \"\"\"\n",
    "    \n",
    "    no_saved_attentions_epochs = np.asarray(attention_maps_temp).shape[0]//sample_number\n",
    "    attentions = np.asarray(attention_maps_temp)\n",
    "    imgs=[]\n",
    "    labels=[]\n",
    "    width_array = (no_saved_attentions_epochs-1)*np.arange(width)//(width-1)\n",
    "    # Prepare the selection of images in the correct order as to be plotted reasonably (and prepare epoch labels)\n",
    "    for j in range(sample_number):\n",
    "        imgs.append(np.exp(source_images[j][0]))\n",
    "        for i in width_array:\n",
    "            #print(i,j,sample_number,len(attention_maps_temp))\n",
    "            imgs.append(attention_maps_temp[sample_number*i+j])\n",
    "            try:\n",
    "                labels[width-1]\n",
    "            except:\n",
    "                labels.append(epoch_labels[sample_number*i])\n",
    "    \n",
    "    # Define the plot of the grid of images\n",
    "    fig = plt.figure(figsize=(40, 40))\n",
    "    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                     nrows_ncols=(sample_number,width+1),#Sets size of array of images\n",
    "                     axes_pad=0.02,  # pad between axes in inch.\n",
    "                     )\n",
    "    for ax, im in zip(grid, imgs):\n",
    "        # Iterating over the grid returns the Axes.\n",
    "        if logged:\n",
    "            ax.imshow(np.log(im),cmap='magma')\n",
    "        else:\n",
    "            ax.imshow(im,cmap='magma')\n",
    "        ax.axis('off')\n",
    "    print(f'Source images followed by their respective averaged attention maps at epochs:\\n{labels}')\n",
    "    plt.savefig(f'TrainingLosses/{ckpt_name}_attention_maps.png')\n",
    "    plt.show()\n",
    "\n",
    "attention_epoch_plot(attention_maps_temp,attention_map_images,sample_number,logged=True,width=5,epoch_labels=epoch_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The below will display a grid similar to that as above, except showing all saved iterations (each improvement of the validation function).\n",
    "print(f'Grid with total x length of 1 source + {len(epoch_updates)//sample_number} attention maps')\n",
    "attention_epoch_plot(attention_maps_temp,attention_map_images,sample_number,logged=False,width=len(epoch_updates)//sample_number,epoch_labels=epoch_updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the model with the lowest loss (highest epoch save no.)\n",
    "\n",
    "#PATH = f'{folder_name}/{i}.pt'\n",
    "#Epoch = 180\n",
    "for epoch_temp in range(5000):\n",
    "    PATH = f'{folder_name}/{epoch_temp}.pt'\n",
    "    if os.path.exists(PATH):\n",
    "        epoch_max = epoch_temp\n",
    "    \n",
    "net.load_state_dict(torch.load(f'{folder_name}/{epoch_max}.pt',map_location=torch.device(device)))\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ignite.metrics\n",
    "classes = ['FRI','FRII']\n",
    "no_classes = 2\n",
    "conf_mat = np.zeros((no_classes,no_classes))\n",
    "correct , total = 0 , 0\n",
    "net.eval()\n",
    "test_results = 0 #forces an except for first iteration.\n",
    "\n",
    "for epoch_count in range(360):\n",
    "    with torch.no_grad():\n",
    "        for data in testset:\n",
    "            X , y = data\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            output = net.forward(X)\n",
    "            \n",
    "            try:\n",
    "                test_results = np.append(test_results,torch.argmax(output,1).cpu().numpy(),axis=0)\n",
    "                test_labels  = np.append(test_labels,y.cpu().numpy(),axis=0)\n",
    "                test_raw_results = np.append(test_raw_results,output.cpu().numpy(),axis=0)\n",
    "            except:\n",
    "                test_results = torch.argmax(output,axis=1).cpu().numpy()\n",
    "                test_labels  = y.cpu().numpy()\n",
    "                test_raw_results = output.cpu().numpy()\n",
    "                #print(f'y={y.shape}{y}\\n output={output.cpu().numpy().shape}{output}\\n output_saved={test_results.shape}{test_results}')\n",
    "                #print('\\n\\ny\\tsaved\\tout')\n",
    "                #for i in range(int(y.cpu().numpy().shape[0])):\n",
    "                #    print(f'{y[i]}\\t{test_results[i]}\\t{output.cpu().numpy()[i]}\\t{test_raw_results[i]}\\n')\n",
    "            #print(test_results)\n",
    "            #print(test_labels)\n",
    "            for idx, i in enumerate(output):\n",
    "                #print(f'idx: {idx:2}; y[idx]: {y[idx]} ; i: {int(torch.argmax(i)):2}')\n",
    "                if int(torch.argmax(i))<1:\n",
    "                    label_predicted_binary = 0\n",
    "                else:\n",
    "                    label_predicted_binary = 1\n",
    "                if y[idx]<5:\n",
    "                    label_true_binary = 0\n",
    "                else:\n",
    "                    label_true_binary = 1\n",
    "                conf_mat[label_true_binary,label_predicted_binary]+=1\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.trace(conf_mat)/conf_mat.sum()*100\n",
    "total = int(conf_mat.sum())\n",
    "correct = int(np.trace(conf_mat))\n",
    "print(f\"Accuracy of Model on test set: {accuracy:.1f}% ({correct} out of {total})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(test_results,test_results.shape)\n",
    "#print(test_raw_results.shape)\n",
    "\n",
    "temp = test_raw_results[:,1]-test_raw_results[:,0]\n",
    "final2 = ((-test_raw_results[:,0]+test_raw_results[:,1])/max(temp.max(),-temp.min())+1)/2\n",
    "\n",
    "test_adapted_results = (temp/max(temp.max(),-temp.min())+1)/2\n",
    "\n",
    "print(f\"\"\"Normalised Classification Values \\elem[0,1]:\n",
    "      \\tMax:\\t{test_adapted_results.max()}\n",
    "      \\tMin:\\t{test_adapted_results.min()}\n",
    "      \\tCount:\\t{test_adapted_results.shape}\"\"\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Histogram of Classification Values\n",
    "plt.figure(figsize=(20,7))\n",
    "plt.subplot(121)\n",
    "plt.hist(test_raw_results,bins=100,histtype='step',alpha=0.7,linewidth=4.0)\n",
    "plt.title(f'Histogram of Raw Classifications ({test_raw_results.shape[0]} Samples)')\n",
    "plt.legend(('FRI','FRII'),loc='best')\n",
    "#plt.text(-3.5,550,f'FRI>FRII classifies as FRI\\nFRI<FRII classifies as FRII')\n",
    "plt.xlabel('Raw CNN Output')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(test_adapted_results,bins=100,histtype='stepfilled',alpha=0.8)\n",
    "plt.title(f'Histogram of Adapted Classifications ({test_adapted_results.shape[0]} Samples)')\n",
    "plt.xlabel('Adapted CNN Output')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, recall_score, f1_score, precision_score\n",
    "auc = []\n",
    "#fpr, tpr, thresholds = roc_curve(test_labels,test_adapted_results)\n",
    "#auc = roc_auc_score(test_labels,test_adapted_results)\n",
    "confusion = confusion_matrix(test_labels,test_results)\n",
    "recall = recall_score(test_labels,test_results,average=None)\n",
    "precision = precision_score(test_labels,test_results,average=None)\n",
    "f1 = f1_score(test_labels,test_results,average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 3 HM Transfer Learning Equivalent Results: (1903.11921.pdf)\n",
    "try:\n",
    "    temp_ = auc/2\n",
    "except:\n",
    "    auc = 'NA'\n",
    "print(f\"\"\"Table 3 HM Transfer Learning Equivalent Results:\n",
    "\n",
    "\\t\\tFRI \\tFRII\n",
    "Recall \\t\\t{recall[0]:.3f} \\t{recall[1]:.3f}\n",
    "Precision \\t{precision[0]:.3f}\\t{precision[1]:.3f}\n",
    "F1 Score \\t{f1[0]:.3f}\\t{f1[1]:.3f}\n",
    "Accuracies \\t{np.sum(confusion[:5,0])/np.sum(confusion[:5])*100:.1f}%\\t{np.sum(confusion[5:,1])/np.sum(confusion[5:])*100:.1f}%\n",
    "\n",
    "Avg. Accuracy \\t{accuracy:.1f}% \\t<-- Not really applicable in multiclass either.\n",
    "AUC \\t\\t{auc} \\t<-- Not Applicable in Multiclass\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix plot for binary selection of RFI_class\n",
    "fig, ax =plt.subplots(figsize=(10,10))\n",
    "#im = ax.imshow(confusion,cmap='Blues',extent=[0,2,0,2])\n",
    "im = ax.imshow(conf_mat,cmap='Blues',extent=[0,2,0,2])\n",
    "\n",
    "print(conf_mat)\n",
    "# Show all ticks\n",
    "ax.set_xticks(np.arange(len(classes)))\n",
    "ax.set_yticks(np.arange(len(classes)))\n",
    "# Label them with the respective list entries\n",
    "ax.set_xticklabels(classes)\n",
    "ax.set_yticklabels([classes[1],classes[0]])\n",
    "\n",
    "ax.set_xlabel('Predicted Label')\n",
    "ax.set_ylabel('True Label')\n",
    "ax.set_title(f\"Confusion Matrix of FRI and FRII\")\n",
    "\n",
    "#Add written out values\n",
    "plt.text(0.45,1.5,str(int(conf_mat[0,0])))\n",
    "plt.text(0.45,0.5,str(int(conf_mat[0,1])))\n",
    "plt.text(1.45,1.5,str(int(conf_mat[1,0])))\n",
    "plt.text(1.45,0.5,str(int(conf_mat[1,1])))\n",
    "plt.text(0.4,-0.05,f'Total: {np.sum(conf_mat[0:1,:]):.0f}')\n",
    "plt.text(1.4,-0.05,f'Total: {np.sum(conf_mat[1:,:]):.0f}')\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "#fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(fpr,tpr)\n",
    "ax.set(xlim=(0, 1), ylim=(0, 1))\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(f'ROC Curve with auc={round(auc,3)}') #Receiver Operating Characteristic\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    # unnormalise images ([-1,1]-->[0,1])\n",
    "    img = img / 2 +0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize = (20,20))\n",
    "    #plt.imshow(np.transpose(npimg,(1,2,0)),'Greys')\n",
    "    plt.imshow(npimg[0],'gray')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (images, labels) in enumerate(testset):\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    outputs = net.forward(images)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    print(f'{predicted}\\n{labels}')\n",
    "    #plt.imshow(images.cpu().numpy()[0,0]/2+0.5)\n",
    "    #plt.colorbar()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(torchvision.utils.make_grid(images.cpu(),padding=2))\n",
    "#for i in range(predicted.shape[0]):\n",
    "#    print(f\"\"\"Image {i+1}: Prediction: \\t{classes[predicted[i]]:4}\\n\\t Truth: \\t{classes[labels[i]]}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = images[0,0].cpu().numpy()\n",
    "input_img = np.expand_dims(input_img, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotNNFilterOverlay(input_img, np.zeros_like(input_img), figure_id=0, interp='bilinear', colormap=cm.jet, title='[GT:{}|P:{}]'.format(labels[0].data, predicted[0].data),alpha=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = \"./plots\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Attention Maps\n",
    "dataiter = iter(testset)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "#print(images.shape,images)\n",
    "attentions, originals = attentions_func(images,net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images.squeeze().shape)\n",
    "images_np = images.squeeze().cpu().numpy()\n",
    "final = np.zeros((4*150,4*150))\n",
    "print(final.shape)\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        final[150*i:150*(i+1),150*j:150*(j+1)] = images[(i*4)+j]\n",
    "\n",
    "#final = final.reshape(150*4,150*4)\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(final*0.5+0.5, cmap='magma')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(attentions.squeeze().shape)\n",
    "attentions_np = attentions.squeeze().transpose(2,0,1)\n",
    "print(attentions_np.shape)\n",
    "final = np.zeros((4*150,4*150))\n",
    "print(final.shape)\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        final[150*i:150*(i+1),150*j:150*(j+1)] = attentions_np[2*(i*4+j)]-attentions_np[2*(i*4+j)+1]/500\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(final, cmap='magma')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate Attention Maps\n",
    "dataiter = iter(outset)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "#print(images.shape,images)\n",
    "attentions, originals = attentions_func(images,net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageNo = 1\n",
    "\n",
    "image4plot = (images[ImageNo][0]+1)/2\n",
    "plt.figure(figsize=(10,10))\n",
    "#plt.imshow(np.log(image4plot+1),cmap='gray')\n",
    "plt.imshow(image4plot,cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.contour(image4plot,3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_attentions_temp = np.mean(attentions[:,:,2*ImageNo:2*ImageNo+2],2)\n",
    "mean_originals = (np.mean(originals[:,:,2*ImageNo:2*ImageNo+2],2))\n",
    "print(mean_originals.shape)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.imshow(np.log(originals[:,:,ImageNo*2]),cmap='magma')\n",
    "plt.title('Log of First Attention Map')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.imshow(np.log(originals[:,:,ImageNo*2+1]),cmap='magma')\n",
    "plt.title('Log of Second Attention Map')\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.imshow(image4plot,cmap='gray')\n",
    "plt.contour(image4plot,3)\n",
    "plt.title('Source')\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.imshow(np.log(mean_originals[:,:]),cmap='magma')\n",
    "#plt.contour(mean_attentions[:,:,0],3)\n",
    "plt.title('Log of Averaged Attention Maps')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images.shape,images[ImageNo].shape)\n",
    "input_img = np.expand_dims(images[ImageNo][0],2)\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(221)\n",
    "plt.imshow(originals[:,:,ImageNo*2],cmap='magma')\n",
    "plt.colorbar()\n",
    "plt.subplot(222)\n",
    "plt.imshow(originals[:,:,ImageNo*2+1],cmap='magma')\n",
    "plt.colorbar()\n",
    "plt.subplot(223)\n",
    "plt.imshow(attentions[:,:,ImageNo*2],cmap='magma')\n",
    "plt.colorbar()\n",
    "plt.subplot(224)\n",
    "plt.imshow(attentions[:,:,ImageNo*2+1],cmap='magma')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotNNFilterOverlay(input_img, \n",
    "                    np.log(np.expand_dims(attentions[:,:,ImageNo*2],2)),\n",
    "                    figure_id=2, interp='bilinear', \n",
    "                    colormap=cm.magma, title='Attention Overlay with Compatability 1', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotNNFilterOverlay(input_img, \n",
    "                    np.expand_dims(attentions[:,:,ImageNo*2+1],2),\n",
    "                    figure_id=2, interp='bilinear', \n",
    "                    colormap=cm.magma, title='Attention Overlay with Compatability 2', alpha=0.5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(attentions[:,:,0])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(attentions[:,:,1])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "print(len(attentions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting average of both attention maps:\n",
    "mean_attentions = np.expand_dims(np.mean(attentions[:,:,2*ImageNo:2*ImageNo+2],2),2)\n",
    "plotNNFilterOverlay(input_img, mean_attentions, figure_id=4, interp='bilinear', colormap='magma', title='Attention Overlay All Compatibilities', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotNNFilterOverlay(input_img, np.log(mean_attentions), figure_id=4, interp='bilinear', colormap='magma', title='Attention Overlay All Compatibilities (logged)', alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_attentions_temp = np.mean(attentions[:,:,2*ImageNo:2*ImageNo+2],2)\n",
    "mean_originals = (np.mean(originals[:,:,2*ImageNo:2*ImageNo+2],2))\n",
    "\n",
    "print(mean_attentions_temp.shape)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.imshow(np.log(originals[:,:,ImageNo*2]),cmap='magma')\n",
    "plt.title('Log of First Attention Map')\n",
    "plt.colorbar()\n",
    "\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.imshow(np.log(originals[:,:,ImageNo*2+1]),cmap='magma')\n",
    "plt.title('Log of Second Attention Map')\n",
    "plt.colorbar()\n",
    "\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.imshow(image4plot,cmap='gray')\n",
    "plt.contour(image4plot,3)\n",
    "plt.title('Source')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.imshow(np.log(mean_attentions_temp),cmap='magma')\n",
    "#plt.contour(mean_attentions[:,:,0],3)\n",
    "plt.title('Log of Averaged Attention Maps')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Continue training the model after it has already been trained.\n",
    "Epoch_extended = 3600\n",
    "#train_net = True\n",
    "continue_training = True\n",
    "\n",
    "if continue_training:\n",
    "    # Variable selections\n",
    "    #valid_loss_min = np.Inf\n",
    "    num_samples = 1 #No. of samples per dataset.\n",
    "    #train_loss_plot=[]\n",
    "    #valid_loss_plot=[]\n",
    "    #min_v_loss_plot=[]\n",
    "    \n",
    "    # Selects most advanced trained model\n",
    "    for epoch_temp in range(Epoch_extended):\n",
    "        PATH = f'{folder_name}/{epoch_temp}.pt'\n",
    "        if os.path.exists(PATH):\n",
    "            loaded_epoch = epoch_temp\n",
    "\n",
    "    print(f'Continuing the training from {loaded_epoch} with minimum validation loss of {min_v_loss_plot[-1]}')\n",
    "    for epoch_count in range(loaded_epoch+1, Epoch_extended):\n",
    "\n",
    "        ### Model Training ###\n",
    "        train_loss = 0.\n",
    "        valid_loss = 0.\n",
    "        net.train() #Set network to train mode.    \n",
    "        for batch_idx , (data, labels) in enumerate(trainset): #Iterates through each batch.        \n",
    "            # Put data on CPU or GPU as available.\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Create binary labels to remove morphological subclassifications\n",
    "            binary_labels = np.zeros(labels.size(),dtype=int)\n",
    "            binary_labels = np.where(labels.cpu().numpy()<5,binary_labels,binary_labels+1)\n",
    "            binary_labels = torch.from_numpy(binary_labels).to(device)\n",
    "\n",
    "            optimizer.zero_grad() # Clear gradients of all optimised variables. ### Should I be doing this? https://pytorch.org/docs/stable/optim.html\n",
    "            outputs = net.forward(data) # Forward pass ie. predicted outputs.\n",
    "            #if type(outputs)==list and net_name[-4]=='mean':\n",
    "            #    outputs = torch.mean(torch.stack(outputs),dim=[0,1])\n",
    "            loss = loss_function(outputs, binary_labels) # Calculate batch loss.\n",
    "\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            train_loss += (loss.item()*data.size(0)) / num_samples\n",
    "\n",
    "        ### Model Validation ###\n",
    "        net.eval()\n",
    "        for epoch_valid in range(validation_epoch):\n",
    "            for batch_idx, (data, labels) in enumerate(validset):\n",
    "                data = data.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Create binary labels to remove morphological subclassifications\n",
    "                binary_labels = np.zeros(labels.size(),dtype=int)\n",
    "                binary_labels = np.where(labels.cpu().numpy()<5,binary_labels,binary_labels+1)\n",
    "                binary_labels = torch.from_numpy(binary_labels).to(device)\n",
    "                \n",
    "                outputs = net.forward(data)\n",
    "                loss = loss_function(outputs, binary_labels)\n",
    "                valid_loss += (loss.item()*data.size(0)) / num_samples\n",
    "\n",
    "        # Average losses (scaled according to validation dataset size)\n",
    "        train_loss = train_loss/(len(trainset.dataset)*(1-valid_size))\n",
    "        valid_loss = valid_loss/(len(validset.dataset)*valid_size*validation_epoch)\n",
    "\n",
    "        # Print\n",
    "        print(f\"Epoch:{epoch_count:3}\\tTraining Loss: {train_loss:8.6f}\\t\\tValidation Loss: {valid_loss:8.6f}\")\n",
    "\n",
    "        # Save model if validation loss decreased (ie. best model with least overfitting)\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print(f\"\\tValidation Loss Down: \\t({valid_loss_min:8.6f}-->{valid_loss:8.6f}) ... Updating saved model.\")\n",
    "            #torch.save(net.state_dict(), f'TrainedNetworks/{ckpt_name}')\n",
    "            ckpt_name_temp = f'{epoch_count}of{Epoch}-vloss{valid_loss:.2f}'\n",
    "            torch.save(net.state_dict(), f'{folder_name}/{epoch_count}.pt')\n",
    "            valid_loss_min = valid_loss\n",
    "\n",
    "        # Save training loss / validation loss for plotting\n",
    "        train_loss_plot.append(train_loss)\n",
    "        valid_loss_plot.append(valid_loss)\n",
    "        min_v_loss_plot.append(valid_loss_min)\n",
    "\n",
    "    print(f\"\\nFinished training.\\nMinimum Validation Loss: {valid_loss_min:8.6}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_net:\n",
    "    plt.figure(figsize=(8,8))\n",
    "\n",
    "    plt.subplot(211)\n",
    "    plt.plot(train_loss_plot)\n",
    "    plt.plot(min_v_loss_plot,'g')\n",
    "    plt.title(f'{net_name} Loss (lr: {learning_rate})')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim(0,1)\n",
    "    plt.grid()\n",
    "    plt.legend(['Training loss','Minimal Validation Loss'])\n",
    "\n",
    "    plt.subplot(212)\n",
    "\n",
    "    plt.plot(train_loss_plot)\n",
    "    plt.plot(valid_loss_plot)\n",
    "    plt.plot(min_v_loss_plot,'g')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.legend(['Training loss','Validation loss','Minimal Validation Loss'])\n",
    "\n",
    "    plt.savefig(f'TrainingLosses/{ckpt_name}_Losses_CONTINUED.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
