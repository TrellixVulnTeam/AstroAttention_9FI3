{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script mostly follows [the standard CIFAR10 Pytorch example](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html). It extracts grey scale images from the dataset.\n",
    "\n",
    "The steps are:\n",
    "\n",
    "1. Load and normalizing the FRDEEP-F training and test datasets using torchvision\n",
    "2. Define a Convolutional Neural Network\n",
    "3. Define a loss function\n",
    "4. Train the network on the training data\n",
    "5. Test the network on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import some standard python libraries for plotting stuff and handling arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then import the pytorch, torchvision and torchsummary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then import the pytorch neural network stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then import the oprimization library from pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally import the FRDEEP-F pytorch dataset class. This is not provided with pytorch, you need to [grab it from the FRDEEP github](\n",
    "https://github.com/HongmingTang060313/FR-DEEP/blob/master/FRDEEP.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FRDEEP import FRDEEPF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks.sononet_ams import sononet\n",
    "from networks.sononet_grid_attention import sononet_grid_attention as sononet2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of torchvision datasets are PILImage images of range [0, 1]. We transform them to Tensors of normalized range [-1, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [torchvision.transforms.RandomRotation(20, resample=PIL.Image.BILINEAR),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize([0.5],[0.5])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the training and test datasets. The first time you do this it will download the data to your working directory, but once the data is there it will just use it without repeating the download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = FRDEEPF(root='./FIRST_data', train=True, download=True, transform=transform)  \n",
    "batch_size_train = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "testset = FRDEEPF(root='./FIRST_data', train=False, download=True, transform=transform) \n",
    "batch_size_test = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain training indices that will be used for validation\n",
    "num_train = len(trainset)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "validset = FRDEEPF(root='./FIRST_data', train=True, download=True, transform=transform)  \n",
    "batch_size_train = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two classes in this dataset: FRI and FRII:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size_train, sampler=train_sampler, num_workers=2)\n",
    "validloader = torch.utils.data.DataLoader(validset, batch_size=batch_size_train, sampler=valid_sampler, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size_test, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('FRI', 'FRII')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little function to display images nicely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    # unnormalize\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at some randomly selected samples to see how they appear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FRII  FRII\n"
     ]
    }
   ],
   "source": [
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(batch_size_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a neural network that takes greyscale images as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 34 * 34, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # conv1 output width: input_width - (kernel_size - 1) => 150 - (5-1) = 146\n",
    "        # pool 1 output width: int(input_width/2) => 73\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # conv2 output width: input_width - (kernel_size - 1) => 73 - (5-1) = 69\n",
    "        # pool 2 output width: int(input_width/2) => 34\n",
    "        x = self.pool(F.relu(self.conv2(x)))  \n",
    "        x = x.view(-1, 16 * 34 * 34)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annascaife/SRC/GITHUB/Attention-Gated-Networks/models/networks_other.py:42: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n",
      "/Users/annascaife/SRC/GITHUB/Attention-Gated-Networks/models/networks_other.py:46: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(m.weight.data, 1.0, 0.02)\n",
      "/Users/annascaife/SRC/GITHUB/Attention-Gated-Networks/models/networks_other.py:47: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(m.bias.data, 0.0)\n",
      "/Users/annascaife/SRC/GITHUB/Attention-Gated-Networks/models/layers/grid_attention_layer.py:269: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  nn.init.constant(self.psi.bias.data, 10.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 150, 150]             160\n",
      "       BatchNorm2d-2         [-1, 16, 150, 150]              32\n",
      "              ReLU-3         [-1, 16, 150, 150]               0\n",
      "            Conv2d-4         [-1, 16, 150, 150]           2,320\n",
      "       BatchNorm2d-5         [-1, 16, 150, 150]              32\n",
      "              ReLU-6         [-1, 16, 150, 150]               0\n",
      "            Conv2d-7         [-1, 16, 150, 150]           2,320\n",
      "       BatchNorm2d-8         [-1, 16, 150, 150]              32\n",
      "              ReLU-9         [-1, 16, 150, 150]               0\n",
      "        unetConv2-10         [-1, 16, 150, 150]               0\n",
      "        MaxPool2d-11           [-1, 16, 75, 75]               0\n",
      "           Conv2d-12           [-1, 32, 75, 75]           4,640\n",
      "      BatchNorm2d-13           [-1, 32, 75, 75]              64\n",
      "             ReLU-14           [-1, 32, 75, 75]               0\n",
      "           Conv2d-15           [-1, 32, 75, 75]           9,248\n",
      "      BatchNorm2d-16           [-1, 32, 75, 75]              64\n",
      "             ReLU-17           [-1, 32, 75, 75]               0\n",
      "           Conv2d-18           [-1, 32, 75, 75]           9,248\n",
      "      BatchNorm2d-19           [-1, 32, 75, 75]              64\n",
      "             ReLU-20           [-1, 32, 75, 75]               0\n",
      "        unetConv2-21           [-1, 32, 75, 75]               0\n",
      "        MaxPool2d-22           [-1, 32, 37, 37]               0\n",
      "           Conv2d-23           [-1, 64, 37, 37]          18,496\n",
      "      BatchNorm2d-24           [-1, 64, 37, 37]             128\n",
      "             ReLU-25           [-1, 64, 37, 37]               0\n",
      "           Conv2d-26           [-1, 64, 37, 37]          36,928\n",
      "      BatchNorm2d-27           [-1, 64, 37, 37]             128\n",
      "             ReLU-28           [-1, 64, 37, 37]               0\n",
      "           Conv2d-29           [-1, 64, 37, 37]          36,928\n",
      "      BatchNorm2d-30           [-1, 64, 37, 37]             128\n",
      "             ReLU-31           [-1, 64, 37, 37]               0\n",
      "        unetConv2-32           [-1, 64, 37, 37]               0\n",
      "        MaxPool2d-33           [-1, 64, 18, 18]               0\n",
      "           Conv2d-34          [-1, 128, 18, 18]          73,856\n",
      "      BatchNorm2d-35          [-1, 128, 18, 18]             256\n",
      "             ReLU-36          [-1, 128, 18, 18]               0\n",
      "           Conv2d-37          [-1, 128, 18, 18]         147,584\n",
      "      BatchNorm2d-38          [-1, 128, 18, 18]             256\n",
      "             ReLU-39          [-1, 128, 18, 18]               0\n",
      "        unetConv2-40          [-1, 128, 18, 18]               0\n",
      "        MaxPool2d-41            [-1, 128, 9, 9]               0\n",
      "           Conv2d-42            [-1, 128, 9, 9]         147,584\n",
      "      BatchNorm2d-43            [-1, 128, 9, 9]             256\n",
      "             ReLU-44            [-1, 128, 9, 9]               0\n",
      "           Conv2d-45            [-1, 128, 9, 9]         147,584\n",
      "      BatchNorm2d-46            [-1, 128, 9, 9]             256\n",
      "             ReLU-47            [-1, 128, 9, 9]               0\n",
      "        unetConv2-48            [-1, 128, 9, 9]               0\n",
      "           Conv2d-49          [-1, 128, 37, 37]           8,192\n",
      "           Conv2d-50            [-1, 128, 9, 9]          16,384\n",
      "           Conv2d-51            [-1, 1, 37, 37]             129\n",
      "GridAttentionBlock2D_TORR-52  [[-1, 64, 37, 37], [-1, 1, 37, 37]]               0\n",
      "           Conv2d-53          [-1, 128, 18, 18]          16,384\n",
      "           Conv2d-54            [-1, 128, 9, 9]          16,384\n",
      "           Conv2d-55            [-1, 1, 18, 18]             129\n",
      "GridAttentionBlock2D_TORR-56  [[-1, 128, 18, 18], [-1, 1, 18, 18]]               0\n",
      "           Linear-57                    [-1, 2]             642\n",
      "================================================================\n",
      "Total params: 696,836\n",
      "Trainable params: 696,836\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.09\n",
      "Forward/backward pass size (MB): 963.90\n",
      "Params size (MB): 2.66\n",
      "Estimated Total Size (MB): 966.64\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annascaife/SRC/GITHUB/p3env/lib/python3.6/site-packages/torch/nn/functional.py:2351: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/Users/annascaife/SRC/GITHUB/p3env/lib/python3.6/site-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    }
   ],
   "source": [
    "#net = Net()\n",
    "net = sononet2(feature_scale=4, n_classes=2, in_channels=1)\n",
    "summary(net,(1,150,150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use Classification Cross-Entropy loss and Adagrad with momentum for optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adagrad(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_sononet_FRDEEP_bayesian.pt'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_name = f'model_sononet_FRDEEP_bayesian.pt'\n",
    "ckpt_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 10 epochs of training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.004499 \tValidation Loss: 0.000514\n",
      "Validation loss decreased (inf --> 0.000514).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.000674 \tValidation Loss: 0.000346\n",
      "Validation loss decreased (0.000514 --> 0.000346).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.000123 \tValidation Loss: 0.000173\n",
      "Validation loss decreased (0.000346 --> 0.000173).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.000055 \tValidation Loss: 0.000065\n",
      "Validation loss decreased (0.000173 --> 0.000065).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.000023 \tValidation Loss: 0.000072\n",
      "Epoch: 6 \tTraining Loss: 0.000011 \tValidation Loss: 0.000051\n",
      "Validation loss decreased (0.000065 --> 0.000051).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.000009 \tValidation Loss: 0.000068\n",
      "Epoch: 8 \tTraining Loss: 0.000009 \tValidation Loss: 0.000049\n",
      "Validation loss decreased (0.000051 --> 0.000049).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.000011 \tValidation Loss: 0.000045\n",
      "Validation loss decreased (0.000049 --> 0.000045).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.000007 \tValidation Loss: 0.000046\n",
      "Finished Training\n",
      "CPU times: user 12min 1s, sys: 2min 41s, total: 14min 43s\n",
      "Wall time: 10min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "batch_size=2\n",
    "outputs = 2\n",
    "inputs = 1\n",
    "resume = False\n",
    "n_epochs = 10\n",
    "lr = 0.001\n",
    "weight_decay = 0.0005\n",
    "num_samples = 1\n",
    "beta_type = \"Blundell\"\n",
    "resize=150\n",
    "use_cuda = False\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    m = math.ceil(len(trainset) / batch_size)\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    net.train()\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        \n",
    "        data = data.view(-1, inputs, resize, resize).repeat(num_samples, 1, 1, 1)\n",
    "        target = target.repeat(num_samples)\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "        if beta_type is \"Blundell\":\n",
    "            beta = 2 ** (m - (batch_idx + 1)) / (2 ** m - 1)\n",
    "        elif beta_type is \"Soenderby\":\n",
    "            beta = min(epoch / (num_epochs // 4), 1)\n",
    "        elif beta_type is \"Standard\":\n",
    "            beta = 1 / m\n",
    "        else:\n",
    "            beta = 0\n",
    "            \n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = net.forward(data)\n",
    "        # calculate the batch loss\n",
    "        #loss = vi(output, target, kl, beta)\n",
    "        loss = vi(outputs, labels)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += (loss.item()*data.size(0)) / num_samples\n",
    "        #print(train_loss)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    net.eval()\n",
    "    for batch_idx, (data, target) in enumerate(validloader):\n",
    "        data = data.view(-1, inputs, resize, resize).repeat(num_samples, 1, 1, 1)\n",
    "        target = target.repeat(num_samples)\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = net.forward(data)\n",
    "        # calculate the batch loss\n",
    "        #loss = vi(output, target, kl, beta)\n",
    "        loss = vi(outputs, labels)\n",
    "        # update average validation loss \n",
    "        valid_loss += (loss.item()*data.size(0)) / num_samples\n",
    "        \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/(len(trainloader.dataset) * (1-valid_size))\n",
    "    valid_loss = valid_loss/(len(validloader.dataset) * valid_size)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(net.state_dict(), ckpt_name)\n",
    "        valid_loss_min = valid_loss\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll try out a couple of test samples just for visual kicks. First load them up and take a look at the true labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:   FRII  FRII\n"
     ]
    }
   ],
   "source": [
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(batch_size_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then see what the network predicts that they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:   FRII  FRII\n"
     ]
    }
   ],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]] for j in range(batch_size_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate the overall accuracy of the network on **all** the test images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 50 test images: 56 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net.forward(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 50 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a imbalanced dataset, so let's take a look at the accuracy for individual classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net.forward(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(batch_size_test):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of   FRI :  0 %\n",
      "Accuracy of  FRII : 100 %\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(classes)):\n",
    "    print('Accuracy of %5s : %2d %%' % (classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
