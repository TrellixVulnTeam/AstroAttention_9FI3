{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Epoch Based Attention**\n",
    "Outline to play around with. Move sections to individual files as planned and required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "#PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "#Torchvision\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "#Other\n",
    "import PIL\n",
    "from torchsummary import summary\n",
    "from FRDEEP import FRDEEPF\n",
    "from models_new import *\n",
    "from models.networks_other import init_weights\n",
    "from skimage.transform import resize\n",
    "\n",
    "#Special Plot Functions\n",
    "import matplotlib.cm as cm\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions / Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will help remove Attention Images as required throughout training\n",
    "class HookBasedFeatureExtractor(nn.Module):\n",
    "    def __init__(self, submodule, layername, upscale=False):\n",
    "        super(HookBasedFeatureExtractor, self).__init__()\n",
    "\n",
    "        self.submodule = submodule\n",
    "        self.submodule.eval()\n",
    "        self.layername = layername\n",
    "        self.outputs_size = None\n",
    "        self.outputs = None\n",
    "        self.inputs = None\n",
    "        self.inputs_size = None\n",
    "        self.upscale = upscale\n",
    "\n",
    "    def get_input_array(self, m, i, o):\n",
    "        if isinstance(i, tuple):\n",
    "            self.inputs = [i[index].data.clone() for index in range(len(i))]\n",
    "            self.inputs_size = [input.size() for input in self.inputs]\n",
    "        else:\n",
    "            self.inputs = i.data.clone()\n",
    "            self.inputs_size = self.input.size()\n",
    "        #print('Input Array Size: ', self.inputs_size)\n",
    "\n",
    "    def get_output_array(self, m, i, o):\n",
    "        if isinstance(o, tuple):\n",
    "            self.outputs = [o[index].data.clone() for index in range(len(o))]\n",
    "            self.outputs_size = [output.size() for output in self.outputs]\n",
    "        else:\n",
    "            self.outputs = o.data.clone()\n",
    "            self.outputs_size = self.outputs.size()\n",
    "        #print('Output Array Size: ', self.outputs_size)\n",
    "\n",
    "    def rescale_output_array(self, newsize):\n",
    "        us = nn.Upsample(size=newsize[2:], mode='bilinear')\n",
    "        if isinstance(self.outputs, list):\n",
    "            for index in range(len(self.outputs)): self.outputs[index] = us(self.outputs[index]).data()\n",
    "        else:\n",
    "            self.outputs = us(self.outputs).data()\n",
    "\n",
    "    def forward(self, x):\n",
    "        target_layer = self.submodule._modules.get(self.layername)\n",
    "\n",
    "        # Collect the output tensor\n",
    "        h_inp = target_layer.register_forward_hook(self.get_input_array)\n",
    "        h_out = target_layer.register_forward_hook(self.get_output_array)\n",
    "        self.submodule(x)\n",
    "        h_inp.remove()\n",
    "        h_out.remove()\n",
    "\n",
    "        # Rescale the feature-map if it's required\n",
    "        if self.upscale: self.rescale_output_array(x.size())\n",
    "\n",
    "        return self.inputs, self.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotNNFilterOverlay(input_im, units, figure_id, interp='bilinear', colormap=cm.viridis, colormap_lim=None, title='', alpha=0.8):\n",
    "    \n",
    "    try:\n",
    "        filters = units.shape[2]\n",
    "    except:\n",
    "        filters = 1\n",
    "    fig = plt.figure(figure_id, figsize=(10,10))\n",
    "    fig.clf()\n",
    "    \n",
    "    for i in range(filters):\n",
    "        plt.imshow(input_im[:,:,0], interpolation=interp, cmap='gray')\n",
    "        plt.imshow(units[:,:,i], interpolation=interp, cmap=colormap, alpha=alpha)\n",
    "        plt.axis('off')\n",
    "        plt.colorbar()\n",
    "        plt.title(title, fontsize='small')\n",
    "        if colormap_lim:\n",
    "            plt.clim(colormap_lim[0],colormap_lim[1])\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My cleaned func to be called at will... Theoretically.\n",
    "def attentions_func(batch_of_images,net):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        batch_of_images: Images with type==torch.tensor, of dimension (-1,1,150,150)\n",
    "        net: model being loaded in.\n",
    "    Calls on: HookBasedFeatureExtractor to call out designed attention maps.\n",
    "    Output: Upscaled Attention Maps, Attention Maps\n",
    "    \n",
    "    \"\"\"\n",
    "    images = batch_of_images\n",
    "    for iteration in range(batch_of_images.shape[0]):\n",
    "        for i in [1,2]:\n",
    "            feature_extractor = HookBasedFeatureExtractor(net, f'compatibility_score{i}', upscale=False)\n",
    "            imap, fmap = feature_extractor.forward(images.to(device)) # imap are the inputs from the class and fmap are the outputs. Both are \n",
    "            #print(imap[0].cpu().numpy().shape)\n",
    "            if not fmap: #Will pass if fmap is none or empty etc. \n",
    "                continue #(ie. Skips iteration if compatibility_score{i} does not compute with the network.)\n",
    "\n",
    "            #print('fmap[0]: ',fmap[0].numpy().shape)\n",
    "            #print('fmap[1]: ',fmap[1].numpy().shape,'This is the attention map! (shape[1]=1)')\n",
    "            attention = fmap[1].cpu().numpy()[iteration,0]\n",
    "\n",
    "            attention = np.expand_dims(resize(attention, (150, 150), mode='edge', preserve_range=True),axis=2)\n",
    "            try:\n",
    "                attentions = np.append(attentions,attention, axis=2)\n",
    "                AMap_originals= np.append(AMap_originals,np.expand_dims(fmap[1].cpu().numpy()[iteration,0],axis=2),axis=2)\n",
    "            except:\n",
    "                attentions = attention\n",
    "                AMap_originals = np.expand_dims(fmap[1].cpu().numpy()[iteration,0],axis=2)\n",
    "    return attentions , AMap_originals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Read in data and prep it (define transformations / batches and download)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([transforms.RandomRotation([0,360],resample=PIL.Image.BILINEAR),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      #transforms.Normalize([0.5],[0.5])\n",
    "                                     ])\n",
    "out_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                     #transforms.Normalize([0.5],[0.5])\n",
    "                                    ])\n",
    "train_transform = transforms.Compose([transforms.RandomRotation([0,360],resample=PIL.Image.BILINEAR),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      #transforms.Normalize([0.5],[0.5])\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define sampling function used to randomly split training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sampler(trainset, valid_size = 0.):\n",
    "    # obtain training indices that will be used for validation\n",
    "    num_train = len(trainset)\n",
    "    indices = list(range(num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    \n",
    "    return (train_sampler, valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "550 samples in the traindata: traindata.shape: 1,torch.Size([1, 150, 150])\n",
      "50 samples in the testdata: testdata.shape: 1,torch.Size([1, 150, 150])\n",
      "50 samples in the outdata: outdata.shape: 1,torch.Size([1, 150, 150])\n"
     ]
    }
   ],
   "source": [
    "#traindata = datasets.MNIST('', train = True, download = True, transform=transform)\n",
    "#testdata = datasets.MNIST('', train = False, download = True, transform = transform)\n",
    "\n",
    "traindata = FRDEEPF(root='./FIRST_data', train=True, download=True, transform=train_transform)\n",
    "testdata = FRDEEPF(root='./FIRST_data', train=False, download=True, transform=test_transform)\n",
    "outdata = FRDEEPF(root='./FIRST_data', train=False, download=True, transform=out_transform)\n",
    "\n",
    "\n",
    "counter=0\n",
    "for data in traindata:\n",
    "    counter+=1\n",
    "print(f'{counter} samples in the traindata: traindata.shape: {data[1]},{data[0].shape}')\n",
    "\n",
    "counter=0\n",
    "for data in testdata:\n",
    "    counter+=1\n",
    "print(f'{counter} samples in the testdata: testdata.shape: {data[1]},{data[0].shape}')\n",
    "\n",
    "counter=0\n",
    "for data in outdata:\n",
    "    counter+=1\n",
    "print(f'{counter} samples in the outdata: outdata.shape: {data[1]},{data[0].shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch, shuffle / sample the test, validation and training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "valid_size = 0.2\n",
    "train_sampler,valid_sampler = Sampler(traindata, valid_size=valid_size)\n",
    "\n",
    "outset = torch.utils.data.DataLoader(outdata, batch_size=batch_size)\n",
    "trainset = torch.utils.data.DataLoader(traindata, batch_size=batch_size, sampler=train_sampler)\n",
    "validset = torch.utils.data.DataLoader(traindata, batch_size=batch_size, sampler=valid_sampler)\n",
    "testset = torch.utils.data.DataLoader(testdata, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Helpful dimensionality visualisation on input values.\n",
    "for batch in outset:\n",
    "    plt.imshow(batch[0][0].view(150,150),cmap='gray')\n",
    "    break\n",
    "#testdata = torch.randn(1,1,150,150)\n",
    "#plt.imshow(testdata[0][0].view(150,150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network\n",
    "Defining our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# This is the one we are acctually using. We call it in sononet_grid_attention.py as AttentionBlock2D. Input Changes:\n",
    "# 'dimension = 2' and 'sub_sample_factor = (1,1)'\n",
    "class _GridAttentionBlock2D_TORR(nn.Module): #Cleaned up from _GridAttentionBlockND_TORR(...)\n",
    "    def __init__(self, in_channels, gating_channels, inter_channels=None, dimension=2, mode='concatenation_softmax',\n",
    "                 sub_sample_factor=(1,1), bn_layer=True, use_W=False, use_phi=True, use_theta=True, use_psi=True, nonlinearity1='relu'):\n",
    "        super(_GridAttentionBlock2D_TORR, self).__init__()\n",
    "\n",
    "        assert dimension in [2] #for 3 dimensional functionality, use original implementation of functions.\n",
    "        assert mode in ['concatenation_softmax'] #Removed all other options for legibility.\n",
    "\n",
    "        # Default parameter set\n",
    "        self.mode = mode\n",
    "        self.dimension = dimension\n",
    "        self.sub_sample_factor = sub_sample_factor if isinstance(sub_sample_factor, tuple) else tuple([sub_sample_factor])*dimension\n",
    "        self.sub_sample_kernel_size = self.sub_sample_factor\n",
    "\n",
    "        # Number of channels\n",
    "        self.in_channels = in_channels\n",
    "        self.gating_channels = gating_channels\n",
    "        self.inter_channels = inter_channels\n",
    "\n",
    "        if self.inter_channels is None: # This is the value we use.\n",
    "            self.inter_channels = in_channels // 2 #Either half of in_channels (in_channels > 1) or = 1 (in_channels = 1).\n",
    "            if self.inter_channels == 0:\n",
    "                self.inter_channels = 1 #We go down to one channel because of this!\n",
    "\n",
    "        if dimension == 2:\n",
    "            conv_nd = nn.Conv2d\n",
    "            bn = nn.BatchNorm2d\n",
    "            self.upsample_mode = 'bilinear'\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "\n",
    "        # initialise id functions\n",
    "        # Theta^T * x_ij + Phi^T * gating_signal + bias\n",
    "        self.W = lambda x: x        # These are essentially base functions for if any of the conditions (below) aren't met,\n",
    "        self.theta = lambda x: x    # in which case each of these methods returns the data without any alterations / augmentations. ie. x -> x\n",
    "        self.psi = lambda x: x\n",
    "        self.phi = lambda x: x\n",
    "        self.nl1 = lambda x: x\n",
    "        \n",
    "        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels, kernel_size=(1,1), stride=(1,1), padding=0, bias=False)\n",
    "        self.phi = conv_nd(in_channels=self.gating_channels, out_channels=self.inter_channels, kernel_size=(1,1), stride=(1,1), padding=0, bias=False)\n",
    "        self.psi = conv_nd(in_channels=self.inter_channels, out_channels=1, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        self.nl1 = lambda x: F.relu(x, inplace=True)\n",
    "\n",
    "        ### Initialise weights using their package (see imports) ###\n",
    "        for m in self.children():\n",
    "            init_weights(m, init_type='kaiming')\n",
    "        #if use_psi and self.mode == 'concatenation_softmax':\n",
    "        nn.init.constant(self.psi.bias.data, 10.0) # Initialises the tensor self.psi.bias.data with values of 10.0 (Because bias=True in initialisation)\n",
    "\n",
    "\n",
    "    def forward(self, x, g):\n",
    "        # As we assert that mode must contain concatenation, this holds for all passes where we pass the initial assertion\n",
    "        #(this was a seperate method, called _concatenation - see sononet_grid_attention.py).\n",
    "        '''\n",
    "        :param x: (b, c, t, h, w),\n",
    "        ie. (batch dim, channel dim, thickness, height, width), in our case we omit thickness as we are working with 2D data.\n",
    "        :param g: (b, g_d)\n",
    "        :return:\n",
    "        '''\n",
    "        input_size = x.size()\n",
    "        batch_size = input_size[0]\n",
    "        assert batch_size == g.size(0)\n",
    "        \n",
    "        # Compute compatibility score: psi_f\n",
    "        #print(x.size())\n",
    "        theta_x = self.theta(x)\n",
    "        theta_x_size = theta_x.size()\n",
    "        \n",
    "        # nl(theta.x + phi.g + bias) -> f = (b, i_c, t, h/s2, w/s3)\n",
    "        phi_g = F.upsample(self.phi(g), size=theta_x_size[2:], mode=self.upsample_mode)\n",
    "        f = theta_x + phi_g\n",
    "        f = self.nl1(f)\n",
    "\n",
    "        psi_f = self.psi(f)\n",
    "\n",
    "        # Calculate Attention map (sigm_psi_f) and weighted output (W_y)\n",
    "        # This block was conditional with: 'self.mode == concatenation_softmax'. Other options are listed in grid_attention_layer.py\n",
    "        # normalisation & scale to x.size()[2:]\n",
    "        # psi^T . f -> (b, 1, t/s1, h/s2, w/s3)\n",
    "        sigm_psi_f = F.softmax(psi_f.view(batch_size, 1, -1), dim=2)\n",
    "        sigm_psi_f = sigm_psi_f.view(batch_size, 1, *theta_x_size[2:])\n",
    "\n",
    "        # sigm_psi_f is attention map! upsample the attentions and multiply\n",
    "        sigm_psi_f = F.upsample(sigm_psi_f, size=input_size[2:], mode=self.upsample_mode) ### mode = bilinear in 2D, ipnut_size is the input WxH of the data.\n",
    "        y = sigm_psi_f.expand_as(x) * x\n",
    "        W_y = self.W(y) #As W = False, W_y = y\n",
    "\n",
    "        return W_y, sigm_psi_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class playgroundv1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(playgroundv1,self).__init__()\n",
    "        filters = [6,16,16,32,64,128]\n",
    "        #ksizes = [11,5,5,3,3,3] # Must all be odd for calculation of padding.\n",
    "        ksizes = [3,3,3,3,3,3] # Must all be odd for calculation of padding.        \n",
    "        self.filters = filters\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1,         out_channels=filters[0],\n",
    "                               kernel_size=ksizes[0],padding=ksizes[0]//2,stride=1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=filters[0],out_channels=filters[1],\n",
    "                               kernel_size=ksizes[1],padding=ksizes[1]//2,stride=1)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=filters[1],out_channels=filters[2],\n",
    "                               kernel_size=ksizes[2],padding=ksizes[2]//2,stride=1)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(in_channels=filters[2],out_channels=filters[3],\n",
    "                               kernel_size=ksizes[3],padding=ksizes[3]//2,stride=1)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(in_channels=filters[3],out_channels=filters[4],\n",
    "                               kernel_size=ksizes[4],padding=ksizes[4]//2,stride=1)\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(in_channels=filters[4],out_channels=filters[5],\n",
    "                               kernel_size=ksizes[5],padding=ksizes[5]//2,stride=1)\n",
    "        \n",
    "        self.mpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.mpool2 = nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        self.mpool3 = nn.MaxPool2d(kernel_size=5, stride=5)\n",
    "        \n",
    "        self.bnorm1 = nn.BatchNorm2d(filters[0])\n",
    "        self.bnorm2 = nn.BatchNorm2d(filters[1])\n",
    "        self.bnorm3 = nn.BatchNorm2d(filters[2])\n",
    "        self.bnorm4 = nn.BatchNorm2d(filters[3])\n",
    "        self.bnorm5 = nn.BatchNorm2d(filters[4])\n",
    "        self.bnorm6 = nn.BatchNorm2d(filters[5])\n",
    "        \n",
    "        self.flatten = nn.Flatten(1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(16*5*5,256) #channel_size * width * height\n",
    "        self.fc2 = nn.Linear(256,256)\n",
    "        self.fc3 = nn.Linear(256,2)\n",
    "        \n",
    "        self.dropout = nn.Dropout()\n",
    "    \n",
    "        ##########\n",
    "        # These are effectively the same calls as in sononet_grid_attention.py due to us redefining the standard selections in _GridAttentionBlock2D(..)        \n",
    "        self.compatibility_score1 = _GridAttentionBlock2D_TORR(in_channels=filters[3] , gating_channels=filters[5], inter_channels=filters[2], use_W=False) ### Why did they choose use_W = False ?\n",
    "        self.compatibility_score2 = _GridAttentionBlock2D_TORR(in_channels=filters[4], gating_channels=filters[5], inter_channels=filters[2], use_W=False)\n",
    "        \n",
    "        # Primary Aggregation selection: (simplest format): ie. >>> if 'concat':\n",
    "        self.classifier = nn.Linear(filters[3]+filters[4]+filters[5], 2)\n",
    "        self.aggregate = self.aggregation_concat\n",
    "        \n",
    "    def aggregation_concat(self, *attended_maps):\n",
    "        #print('TORCH CATINATION GRAPHS: ',torch.cat(attended_maps,dim=1).shape)\n",
    "        return self.classifier(torch.cat(attended_maps, dim=1))\n",
    "        ##########\n",
    "    \n",
    "    # Activation function\n",
    "    @staticmethod\n",
    "    def apply_argmax_softmax(pred):\n",
    "        log_p = F.softmax(pred, dim=1)\n",
    "        return log_p\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        conv1  = F.relu(self.conv1(inputs))\n",
    "        bnorm1 = self.bnorm1(conv1)\n",
    "        mpool1 = self.mpool1(bnorm1)\n",
    "        \n",
    "        conv2  = F.relu(self.conv2(mpool1))\n",
    "        bnorm2 = self.bnorm2(conv2)\n",
    "        mpool2 = self.mpool2(bnorm2)\n",
    "        \n",
    "        ###\n",
    "        # Arbitrarily added to extend size of network (goal: hopefully better classification - seemed to not be 'learning')\n",
    "        conv3  = F.relu(self.conv3(mpool2))\n",
    "        bnorm3 = self.bnorm3(conv3)\n",
    "        #mpool2 = bnorm2\n",
    "        \n",
    "        conv3  = F.relu(self.conv3(bnorm3))\n",
    "        bnorm3 = self.bnorm3(conv3)\n",
    "        \n",
    "        conv3  = F.relu(self.conv3(bnorm3))\n",
    "        bnorm3 = self.bnorm3(conv3)\n",
    "        mpool3 = bnorm2\n",
    "        ###\n",
    "        \n",
    "        conv4  = F.relu(self.conv4(mpool3))\n",
    "        bnorm4 = self.bnorm4(conv4)\n",
    "        \n",
    "        conv5  = F.relu(self.conv5(bnorm4))\n",
    "        bnorm5 = self.bnorm5(conv5)\n",
    "        \n",
    "        conv6  = F.relu(self.conv6(bnorm5))\n",
    "        bnorm6 = self.bnorm6(conv6)\n",
    "        mpool3 = self.mpool3(bnorm6)\n",
    "        \n",
    "        #flatten = self.flatten(mpool3)\n",
    "        #fc1     = F.relu(self.fc1(flatten))\n",
    "        #do      = self.dropout(fc1)\n",
    "        #fc2     = F.relu(self.fc2(do))\n",
    "        #do      = self.dropout(fc2)\n",
    "        #fc2     = F.relu(self.fc2(do))\n",
    "        #do      = self.dropout(fc2)\n",
    "        #fc3     = F.relu(self.fc3(do))\n",
    "        \n",
    "        # Applied Attention , Attention map\n",
    "        #We use conv5 as the global attention as this is the most advanced stage (upsampled to conv3/conv4 to make it work).\n",
    "        #(inputs are before maxpool layer in sononet_grid_attention.py)\n",
    "        #print(conv3.size(),conv4.size(),conv5.size())\n",
    "        #print('4, 5 and 6 shapes: \\n', conv4.shape,conv5.shape,conv6.shape)\n",
    "        attendedConv3 , atten3 = self.compatibility_score1(conv4, conv6)\n",
    "        attendedConv4 , atten4 = self.compatibility_score2(conv5, conv6)\n",
    "        \n",
    "        filters = self.filters\n",
    "        batch_size = inputs.shape[0]\n",
    "        \n",
    "        # Aggregation\n",
    "        pooled = F.adaptive_avg_pool2d(conv6,(1,1)).view(batch_size,-1)\n",
    "        g1 = torch.sum(attendedConv3.view(batch_size, filters[3], -1), dim=-1)\n",
    "        g2 = torch.sum(attendedConv4.view(batch_size, filters[4], -1), dim=-1)\n",
    "        \n",
    "        #print('TOTAL SIZE OF LINAR FUNCTION SHOULD BE: ',filters[4]+2*filters[5])\n",
    "        #print('G1 , G2 and POOLED shape: ',g1.shape,g2.shape,pooled.shape)\n",
    "        out = self.aggregate(g1,g2,pooled)\n",
    "        \n",
    "        \n",
    "        #return F.log_softmax(out,dim=1)\n",
    "        return F.softmax(out,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will be putting our network and data on >> cpu <<\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"We will be putting our network and data on >> {device} <<\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select network from any of the listed options, move it to the gpu if available and then print the summary of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mBowles/Attention-MSc/PYTHON/models/networks_other.py:42: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n",
      "/Users/mBowles/Attention-MSc/PYTHON/models_new/playground_v1.py:199: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  nn.init.constant(self.psi.bias.data, 10.0) # Initialises the tensor self.psi.bias.data with values of 10.0 (Because bias=True in initialisation)\n"
     ]
    }
   ],
   "source": [
    "from models_new import *\n",
    "\n",
    "#net = playground(); net_name = 'playground'\n",
    "net = playgroundv1(); net_name = 'playgroundv1'\n",
    "#net = transfer_original(); net_name = 'transfer_original'\n",
    "#net = AGSononet(); net_name = 'AGSononet'\n",
    "#net = AGTransfer(); net_name = 'AGTransfer'\n",
    "\n",
    "# Put network on device defined previously.\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 6, 150, 150]              60\n",
      "       BatchNorm2d-2          [-1, 6, 150, 150]              12\n",
      "         MaxPool2d-3            [-1, 6, 75, 75]               0\n",
      "            Conv2d-4           [-1, 16, 75, 75]             880\n",
      "       BatchNorm2d-5           [-1, 16, 75, 75]              32\n",
      "         MaxPool2d-6           [-1, 16, 25, 25]               0\n",
      "            Conv2d-7           [-1, 16, 25, 25]           2,320\n",
      "       BatchNorm2d-8           [-1, 16, 25, 25]              32\n",
      "            Conv2d-9           [-1, 16, 25, 25]           2,320\n",
      "      BatchNorm2d-10           [-1, 16, 25, 25]              32\n",
      "           Conv2d-11           [-1, 16, 25, 25]           2,320\n",
      "      BatchNorm2d-12           [-1, 16, 25, 25]              32\n",
      "           Conv2d-13           [-1, 32, 75, 75]           4,640\n",
      "      BatchNorm2d-14           [-1, 32, 75, 75]              64\n",
      "           Conv2d-15           [-1, 64, 75, 75]          18,496\n",
      "      BatchNorm2d-16           [-1, 64, 75, 75]             128\n",
      "           Conv2d-17          [-1, 128, 75, 75]          73,856\n",
      "      BatchNorm2d-18          [-1, 128, 75, 75]             256\n",
      "        MaxPool2d-19          [-1, 128, 15, 15]               0\n",
      "           Conv2d-20           [-1, 16, 75, 75]             512\n",
      "           Conv2d-21           [-1, 16, 75, 75]           2,048\n",
      "           Conv2d-22            [-1, 1, 75, 75]              17\n",
      "_GridAttentionBlock2D_TORR-23  [[-1, 32, 75, 75], [-1, 1, 75, 75]]               0\n",
      "           Conv2d-24           [-1, 16, 75, 75]           1,024\n",
      "           Conv2d-25           [-1, 16, 75, 75]           2,048\n",
      "           Conv2d-26            [-1, 1, 75, 75]              17\n",
      "_GridAttentionBlock2D_TORR-27  [[-1, 64, 75, 75], [-1, 1, 75, 75]]               0\n",
      "           Linear-28                    [-1, 2]             450\n",
      "================================================================\n",
      "Total params: 111,596\n",
      "Trainable params: 111,596\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.09\n",
      "Forward/backward pass size (MB): 23147.78\n",
      "Params size (MB): 0.43\n",
      "Estimated Total Size (MB): 23148.29\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mBowles/attenv/lib/python3.7/site-packages/torch/nn/functional.py:2390: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/Users/mBowles/attenv/lib/python3.7/site-packages/torch/nn/functional.py:2479: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    }
   ],
   "source": [
    "summary(net, (1,150,150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Defining optimizer, loss_function, epoch, training etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch = 360\n",
    "validation_epoch = 360 #No. of transformations made on validation set to create total validation set.\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "learning_rate = 1.0e-3 #Adagrad: 1.0e-(5,6,7) didnt work #Adam: 1.0e-4 didnt work\n",
    "optimizer = optim.Adagrad(net.parameters(), lr=learning_rate) #Best lr: 1.0e-3 (so far)\n",
    "#optimizer = optim.Adadelta(net.parameters()) ### lr = 1.0 standard <-- https://pytorch.org/docs/stable/optim.html\n",
    "#optimizer = optim.Adam(net.parameters(),lr=learning_rate) # lr = 0.001 standard #Best lr: 1.0e-5 (so far - doesnt go past 0.5 for 360 epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Saved Under TrainedNetworks/playgroundv1-0224-0.001_360Epochs.pt\n",
      "Directory to save model already exists (True).\n"
     ]
    }
   ],
   "source": [
    "date = '0224' #format: mmdd\n",
    "ckpt_name = f\"{net_name}-{date}-{learning_rate}_{Epoch}Epochs.pt\"\n",
    "folder_name = f\"TrainedNetworks/{date}-{net_name}\"\n",
    "print(f\"Final Model Saved Under TrainedNetworks/{ckpt_name}\")\n",
    "try:\n",
    "    os.mkdir(folder_name)\n",
    "    print(f'Directory to save model created as: {os.path.isdir(folder_name)}')\n",
    "except:\n",
    "    print(f'Directory to save model already exists ({os.path.isdir(folder_name)}).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded network from: TrainedNetworks/0224-playgroundv1/277.pt\n",
      "CPU times: user 3.22 ms, sys: 1.53 ms, total: 4.75 ms\n",
      "Wall time: 5.04 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_net = False\n",
    "\n",
    "if train_net:\n",
    "    # Variable selections\n",
    "    valid_loss_min = np.Inf\n",
    "    num_samples = 1 #No. of samples per dataset.\n",
    "    train_loss_plot=[]\n",
    "    valid_loss_plot=[]\n",
    "    min_v_loss_plot=[]\n",
    "    #attention_maps_temp=[]\n",
    "    #epoch_updates=[]\n",
    "    #print(f'model located on: {net.device}')\n",
    "\n",
    "    for epoch_count in range(Epoch):\n",
    "\n",
    "        ### Model Training ###\n",
    "        train_loss = 0.\n",
    "        valid_loss = 0.\n",
    "        net.train() #Set network to train mode.    \n",
    "        for batch_idx , (data, labels) in enumerate(trainset): #Iterates through each batch.        \n",
    "            # Put data on CPU or GPU as available.\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad() # Clear gradients of all optimised variables. ### Should I be doing this? https://pytorch.org/docs/stable/optim.html\n",
    "            outputs = net.forward(data) # Forward pass ie. predicted outputs.\n",
    "            loss = loss_function(outputs, labels) # Calculate batch loss.\n",
    "\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            train_loss += (loss.item()*data.size(0)) / num_samples\n",
    "\n",
    "        ### Model Validation ###\n",
    "        net.eval()\n",
    "        for epoch_valid in range(validation_epoch):\n",
    "            for batch_idx, (data, labels) in enumerate(validset):\n",
    "                data = data.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = net.forward(data)\n",
    "                loss = loss_function(outputs, labels)\n",
    "                valid_loss += (loss.item()*data.size(0)) / num_samples\n",
    "\n",
    "        # Average losses (scaled according to validation dataset size)\n",
    "        train_loss = train_loss/(len(trainset.dataset)*(1-valid_size))\n",
    "        valid_loss = valid_loss/(len(validset.dataset)*valid_size*validation_epoch)\n",
    "\n",
    "        # Print\n",
    "        print(f\"Epoch:{epoch_count:3}\\tTraining Loss: {train_loss:8.6f}\\t\\tValidation Loss: {valid_loss:8.6f}\")\n",
    "\n",
    "        # Save model if validation loss decreased (ie. best model with least overfitting)\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print(f\"\\tValidation Loss Down: \\t({valid_loss_min:8.6f}-->{valid_loss:8.6f}) ... Updating saved model.\")\n",
    "            #torch.save(net.state_dict(), f'TrainedNetworks/{ckpt_name}')\n",
    "            ckpt_name_temp = f'{epoch_count}of{Epoch}-vloss{valid_loss:.2f}'\n",
    "            torch.save(net.state_dict(), f'{folder_name}/{epoch_count}.pt')\n",
    "            valid_loss_min = valid_loss\n",
    "\n",
    "            # Pass through output data to visualise attention map learning:\n",
    "            # Calculate Attention maps, average, append\n",
    "            #attentions , originals = attentions_func(torch.tensor(np.asarray(attention_map_images)))\n",
    "            #for i in range(attentions.shape[2]//2):\n",
    "            #    mean_attentions_temp = np.mean(attentions[:,:,i*2:i*2+1],2)\n",
    "            #    attention_maps_temp.append(mean_attentions_temp) # Averaged attention maps of the images selected in the cell above.\n",
    "            #    epoch_updates.append(epoch_count) #List of when the validation loss / attention maps were updated.\n",
    "\n",
    "\n",
    "        # Save training loss / validation loss for plotting\n",
    "        train_loss_plot.append(train_loss)\n",
    "        valid_loss_plot.append(valid_loss)\n",
    "        min_v_loss_plot.append(valid_loss_min)\n",
    "\n",
    "    print(f\"\\nFinished training.\\nMinimum Validation Loss: {valid_loss_min:8.6}\\n\")\n",
    "\n",
    "else:\n",
    "    # Load the model with the lowest loss (highest epoch save no.)\n",
    "    for epoch_temp in range(Epoch):\n",
    "        PATH = f'{folder_name}/{epoch_temp}.pt'\n",
    "        if os.path.exists(PATH):\n",
    "            epoch_max = epoch_temp\n",
    "\n",
    "    net.load_state_dict(torch.load(f'{folder_name}/{epoch_max}.pt',map_location=torch.device(device)))\n",
    "    net.eval()\n",
    "    print(f'Loaded network from: {folder_name}/{epoch_max}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_net:\n",
    "    plt.figure(figsize=(8,8))\n",
    "\n",
    "    plt.subplot(211)\n",
    "    plt.plot(train_loss_plot)\n",
    "    plt.plot(min_v_loss_plot,'g')\n",
    "    plt.title(f'{net_name} Loss (lr: {learning_rate})')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.legend(['Training loss','Minimal Validation Loss'])\n",
    "\n",
    "    plt.subplot(212)\n",
    "\n",
    "    plt.plot(train_loss_plot)\n",
    "    plt.plot(valid_loss_plot)\n",
    "    plt.plot(min_v_loss_plot,'g')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.legend(['Training loss','Validation loss','Minimal Validation Loss'])\n",
    "\n",
    "    plt.savefig(f'TrainingLosses/{ckpt_name}_Losses.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#np.save('attention_maps_temp_testing',np.asarray(attention_maps_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which Images will be used for epoch plot of attentions:\n",
    "sample_number = 50\n",
    "attention_map_images = []\n",
    "attention_map_labels = []\n",
    "\n",
    "# Read in my outset data (same as test set, but with no transformations)\n",
    "images = [] #Forces try except on every first iteration.\n",
    "dataiter = iter(outset)\n",
    "for data in dataiter:\n",
    "    images_temp , labels_temp = data\n",
    "    try:\n",
    "        images , labels = torch.cat((images_temp, images)), torch.cat((labels,labels_temp))\n",
    "    except:\n",
    "        images , labels = images_temp, labels_temp\n",
    "\n",
    "for i in range(-sample_number//2,sample_number//2):\n",
    "    attention_map_images.append(images.cpu().numpy()[i]) # Used as input for hook based extractor (attentions_func)\n",
    "    attention_map_labels.append(labels.cpu().numpy()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 150, 150)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(attention_map_images[0].shape)\n",
    "plt.imshow(attention_map_images[6][0])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mAttentionImagesByEpoch\u001b[0;34m(sources, folder_name, net, epoch)\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-0c48b90ebc9e>\u001b[0m in \u001b[0;36mattentions_func\u001b[0;34m(batch_of_images, net)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mfeature_extractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHookBasedFeatureExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'compatibility_score{i}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mimap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# imap are the inputs from the class and fmap are the outputs. Both are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0;31m#print(imap[0].cpu().numpy().shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfmap\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#Will pass if fmap is none or empty etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-c3b8a925b3b4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mh_inp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_forward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_input_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mh_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_forward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mh_inp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mh_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/attenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Attention-MSc/PYTHON/models_new/playground_v1.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mconv6\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbnorm5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mbnorm6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbnorm6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mmpool3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmpool3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbnorm6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m#flatten = self.flatten(mpool3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/attenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/attenv/lib/python3.7/site-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[1;32m    140\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                             self.return_indices)\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/attenv/lib/python3.7/site-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/attenv/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m     return torch.max_pool2d(\n\u001b[0;32m--> 487\u001b[0;31m         input, kernel_size, stride, padding, dilation, ceil_mode)\n\u001b[0m\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m max_pool2d = boolean_dispatch(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def AttentionImagesByEpoch(sources,folder_name,net,epoch=360):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sources: list of Images with type==torch.tensor, of dimension (-1,1,150,150)\n",
    "        folder_name: directory of pickled .pt parameters to load into our network.\n",
    "    dependancies:\n",
    "        attentions_func()\n",
    "        HookedBasedFeatureExtraction() (from within attention_func)\n",
    "    out:\n",
    "        attention_maps_temp: array of all attention maps according to the epoch they were generated.\n",
    "        epoch_updates: list of epoch numbers for the attention map generations.\n",
    "    \"\"\"\n",
    "    \n",
    "    attention_maps_temp=[]\n",
    "    epoch_updates=[]\n",
    "\n",
    "    # Load in models in improving order based on the folder name\n",
    "    for epoch_temp in range(epoch):\n",
    "        PATH = f'{folder_name}/{epoch_temp}.pt'\n",
    "        if os.path.exists(PATH):\n",
    "            net.load_state_dict(torch.load(PATH,map_location=torch.device(device)))\n",
    "            net.eval()\n",
    "            # Generate attention maps with attentions_func and save appropriately.\n",
    "            attentions , attentions_originals_temp = attentions_func(torch.tensor(np.asarray(attention_map_images)),net)\n",
    "            for i in range(attentions.shape[2]//2):\n",
    "                mean_attentions_temp = np.mean(attentions[:,:,i*2:i*2+1],2)\n",
    "                attention_maps_temp.append(mean_attentions_temp) # Averaged attention maps of the images selected in the cell above.\n",
    "                epoch_updates.append(epoch_temp) #List of when the validation loss / attention maps were updated.        \n",
    "\n",
    "    return attention_maps_temp , epoch_updates\n",
    "\n",
    "attention_maps_temp , epoch_updates = AttentionImagesByEpoch(attention_map_images,folder_name,net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'attention_maps_temp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'attention_maps_temp' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def attention_epoch_plot(attention_maps_temp, \n",
    "                         source_images=attention_map_images,\n",
    "                         sample_number=8,\n",
    "                         logged=True,\n",
    "                         width=3,\n",
    "                         epoch_labels=['No epoch listings provided'],\n",
    "                        ):\n",
    "    \"\"\"\n",
    "    Function purely for plotting clean grid of attention maps as they develop throughout the learning stages.\n",
    "    Args:\n",
    "        The attention map data, \n",
    "        original images of sources\n",
    "        number of unique sources, \n",
    "        if you want your image logged,\n",
    "        number of output attentions desired (sampled evenly accross available space)\n",
    "        epoch labels of when the images were extracted\n",
    "    Out:\n",
    "        plt of images concatenated in correct fashion\n",
    "    \"\"\"\n",
    "    \n",
    "    no_saved_attentions_epochs = np.asarray(attention_maps_temp).shape[0]//sample_number\n",
    "    attentions = np.asarray(attention_maps_temp)\n",
    "    imgs=[]\n",
    "    labels=[]\n",
    "    width_array = (no_saved_attentions_epochs-1)*np.arange(width)//(width-1)\n",
    "    # Prepare the selection of images in the correct order as to be plotted reasonably (and prepare epoch labels)\n",
    "    for j in range(sample_number):\n",
    "        imgs.append(np.exp(source_images[j][0]))\n",
    "        for i in width_array:\n",
    "            imgs.append(attention_maps_temp[sample_number*i+j])\n",
    "            try:\n",
    "                labels[width-1]\n",
    "            except:\n",
    "                labels.append(epoch_labels[sample_number*i])\n",
    "    \n",
    "    # Define the plot of the grid of images\n",
    "    fig = plt.figure(figsize=(40, 40))\n",
    "    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                     nrows_ncols=(sample_number,width+1),#Sets size of array of images\n",
    "                     axes_pad=0.02,  # pad between axes in inch.\n",
    "                     )\n",
    "    for ax, im in zip(grid, imgs):\n",
    "        # Iterating over the grid returns the Axes.\n",
    "        if logged:\n",
    "            ax.imshow(np.log(im),cmap='magma')\n",
    "        else:\n",
    "            ax.imshow(im,cmap='magma')\n",
    "        ax.axis('off')\n",
    "    print(f'Source images followed by their respective averaged attention maps at epochs:\\n{labels}')\n",
    "    plt.savefig(f'TrainingLosses/{ckpt_name}_attention_maps.png')\n",
    "    plt.show()\n",
    "\n",
    "attention_epoch_plot(attention_maps_temp,attention_map_images,sample_number,logged=True,width=5,epoch_labels=epoch_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch_updates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-807c29da06e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# The below will display a grid similar to that as above, except showing all saved iterations (each improvement of the validation function).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Grid with total x length of 1 source + {len(epoch_updates)//sample_number} attention maps'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#attention_epoch_plot(attention_maps_temp,attention_map_images,sample_number,logged=True,width=len(epoch_updates)//sample_number,epoch_labels=epoch_updates)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epoch_updates' is not defined"
     ]
    }
   ],
   "source": [
    "# The below will display a grid similar to that as above, except showing all saved iterations (each improvement of the validation function).\n",
    "print(f'Grid with total x length of 1 source + {len(epoch_updates)//sample_number} attention maps')\n",
    "#attention_epoch_plot(attention_maps_temp,attention_map_images,sample_number,logged=True,width=len(epoch_updates)//sample_number,epoch_labels=epoch_updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with the lowest loss (highest epoch save no.)\n",
    "\n",
    "#PATH = f'{folder_name}/{i}.pt'\n",
    "for epoch_temp in range(Epoch):\n",
    "    PATH = f'{folder_name}/{epoch_temp}.pt'\n",
    "    if os.path.exists(PATH):\n",
    "        epoch_max = epoch_temp\n",
    "    \n",
    "net.load_state_dict(torch.load(f'{folder_name}/{epoch_max}.pt',map_location=torch.device(device)))\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ignite.metrics\n",
    "classes = ['FRI','FRII']\n",
    "conf_mat = np.zeros((2,2))\n",
    "correct , total = 0 , 0\n",
    "net.eval()\n",
    "test_results = 0 #forces an except for first iteration.\n",
    "\n",
    "for epoch_count in range(Epoch):\n",
    "    with torch.no_grad():\n",
    "        for data in testset:\n",
    "            X , y = data\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            output = net.forward(X)\n",
    "            \n",
    "            try:\n",
    "                test_results = np.append(test_results,torch.argmax(output,1).cpu().numpy(),axis=0)\n",
    "                test_labels  = np.append(test_labels,y.cpu().numpy(),axis=0)\n",
    "                test_raw_results = np.append(test_raw_results,output.cpu().numpy(),axis=0)\n",
    "            except:\n",
    "                test_results = torch.argmax(output,axis=1).cpu().numpy()\n",
    "                test_labels  = y.cpu().numpy()\n",
    "                test_raw_results = output.cpu().numpy()\n",
    "                #print(f'y={y.shape}{y}\\n output={output.cpu().numpy().shape}{output}\\n output_saved={test_results.shape}{test_results}')\n",
    "                #print('\\n\\ny\\tsaved\\tout')\n",
    "                #for i in range(int(y.cpu().numpy().shape[0])):\n",
    "                #    print(f'{y[i]}\\t{test_results[i]}\\t{output.cpu().numpy()[i]}\\t{test_raw_results[i]}\\n')\n",
    "                \n",
    "            for idx, i in enumerate(output):\n",
    "                conf_mat[y[idx],int(torch.argmax(i))]+=1\n",
    "\n",
    "accuracy = (conf_mat[0,0]+conf_mat[1,1])/conf_mat.sum()*100\n",
    "total = int(conf_mat.sum())\n",
    "correct = int(conf_mat[0,0]+conf_mat[1,1])\n",
    "print(f\"Accuracy of Model on test set: {accuracy:.1f}% ({correct} out of {total})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(test_results,test_results.shape)\n",
    "#print(test_raw_results.shape)\n",
    "\n",
    "temp = test_raw_results[:,1]-test_raw_results[:,0]\n",
    "final2 = ((-test_raw_results[:,0]+test_raw_results[:,1])/max(temp.max(),-temp.min())+1)/2\n",
    "\n",
    "test_adapted_results = (temp/max(temp.max(),-temp.min())+1)/2\n",
    "\n",
    "print(f\"\"\"Normalised Classification Values \\elem[0,1]:\n",
    "      \\tMax:\\t{test_adapted_results.max()}\n",
    "      \\tMin:\\t{test_adapted_results.min()}\n",
    "      \\tCount:\\t{test_adapted_results.shape}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Classification Values\n",
    "plt.figure(figsize=(20,7))\n",
    "plt.subplot(121)\n",
    "plt.hist(test_raw_results,bins=100,histtype='step',alpha=0.7,linewidth=4.0)\n",
    "plt.title(f'Histogram of Raw Classifications ({test_raw_results.shape[0]} Samples)')\n",
    "plt.legend(('FRI','FRII'),loc='best')\n",
    "#plt.text(-3.5,550,f'FRI>FRII classifies as FRI\\nFRI<FRII classifies as FRII')\n",
    "plt.xlabel('Raw CNN Output')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(test_adapted_results,bins=100,histtype='stepfilled',alpha=0.8)\n",
    "plt.title(f'Histogram of Adapted Classifications ({test_adapted_results.shape[0]} Samples)')\n",
    "plt.xlabel('Adapted CNN Output')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, recall_score, f1_score, precision_score\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_labels,test_adapted_results)\n",
    "auc = roc_auc_score(test_labels,test_adapted_results)\n",
    "confusion = confusion_matrix(test_labels,test_results)\n",
    "recall = recall_score(test_labels,test_results,average=None)\n",
    "precision = precision_score(test_labels,test_results,average=None)\n",
    "f1 = f1_score(test_labels,test_results,average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 3 HM Transfer Learning Equivalent Results: (1903.11921.pdf)\n",
    "print(f\"\"\"Table 3 HM Transfer Learning Equivalent Results:\n",
    "\n",
    "\\t\\tFRI \\tFRII\n",
    "Recall \\t\\t{recall[0]:.3f} \\t{recall[1]:.3f}\n",
    "Precision \\t{precision[0]:.3f}\\t{precision[1]:.3f}\n",
    "F1 Score \\t{f1[0]:.3f}\\t{f1[1]:.3f}\n",
    "Accuracies \\t{confusion[0,0]/np.sum(confusion[0])*100:.1f}%\\t{confusion[1,1]/np.sum(confusion[1])*100:.1f}%\n",
    "\n",
    "Avg. Accuracy \\t{accuracy:.1f}%\n",
    "AUC \\t\\t{auc:.3f}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix plot for binary selection of RFI_class\n",
    "fig, ax =plt.subplots(figsize=(10,10))\n",
    "im = ax.imshow(confusion,cmap='Blues',extent=[0,2,0,2])\n",
    "#fig.colorbar(im)\n",
    "\n",
    "# Show all ticks\n",
    "ax.set_xticks(np.arange(len(classes)))\n",
    "ax.set_yticks(np.arange(len(classes)))\n",
    "# Label them with the respective list entries\n",
    "ax.set_xticklabels([classes[1],classes[0]])\n",
    "ax.set_yticklabels(classes)\n",
    "\n",
    "ax.set_xlabel('True')\n",
    "ax.set_ylabel('Predicted')\n",
    "ax.set_title(f\"Confusion Matrix of FRI and FRII\")\n",
    "\n",
    "#Add written out values\n",
    "plt.text(0.45,1.5,str(confusion[0,0]))\n",
    "plt.text(0.45,0.5,str(confusion[0,1]))\n",
    "plt.text(1.45,0.5,str(confusion[1,1]))\n",
    "plt.text(1.45,1.5,str(confusion[1,0]))\n",
    "plt.text(0.4,-0.05,f'Total: {confusion[0,0]+confusion[0,1]}')\n",
    "plt.text(1.4,-0.05,f'Total: {confusion[1,0]+confusion[1,1]}')\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "#fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(fpr,tpr)\n",
    "ax.set(xlim=(0, 1), ylim=(0, 1))\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(f'ROC Curve with auc={round(auc,3)}') #Receiver Operating Characteristic\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    # unnormalise images ([-1,1]-->[0,1])\n",
    "    img = img / 2 +0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize = (20,20))\n",
    "    #plt.imshow(np.transpose(npimg,(1,2,0)),'Greys')\n",
    "    plt.imshow(npimg[0],'gray')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (images, labels) in enumerate(testset):\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    outputs = net.forward(images)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    print(f'{predicted}\\n{labels}')\n",
    "    #plt.imshow(images.cpu().numpy()[0,0]/2+0.5)\n",
    "    #plt.colorbar()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(torchvision.utils.make_grid(images.cpu(),padding=2))\n",
    "#for i in range(predicted.shape[0]):\n",
    "#    print(f\"\"\"Image {i+1}: Prediction: \\t{classes[predicted[i]]:4}\\n\\t Truth: \\t{classes[labels[i]]}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = images[0,0].cpu().numpy()\n",
    "input_img = np.expand_dims(input_img, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotNNFilterOverlay(input_img, np.zeros_like(input_img), figure_id=0, interp='bilinear', colormap=cm.jet, title='[GT:{}|P:{}]'.format(labels[0].data, predicted[0].data),alpha=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = \"./plots\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate Attention Maps\n",
    "dataiter = iter(outset)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "#print(images.shape,images)\n",
    "attentions, originals = attentions_func(images,net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageNo = 0\n",
    "\n",
    "image4plot = (images[ImageNo][0]+1)/2\n",
    "plt.figure(figsize=(10,10))\n",
    "#plt.imshow(np.log(image4plot+1),cmap='gray')\n",
    "plt.imshow(image4plot,cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.contour(image4plot,3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_attentions_temp = np.mean(attentions[:,:,2*ImageNo:2*ImageNo+2],2)\n",
    "mean_originals = (np.mean(originals[:,:,2*ImageNo:2*ImageNo+2],2))\n",
    "print(mean_originals.shape)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.imshow(np.log(originals[:,:,ImageNo*2]),cmap='magma')\n",
    "plt.title('Log of First Attention Map')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.imshow(np.log(originals[:,:,ImageNo*2+1]),cmap='magma')\n",
    "plt.title('Log of Second Attention Map')\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.imshow(image4plot,cmap='gray')\n",
    "plt.contour(image4plot,3)\n",
    "plt.title('Source')\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.imshow(np.log(mean_originals[:,:]),cmap='magma')\n",
    "#plt.contour(mean_attentions[:,:,0],3)\n",
    "plt.title('Log of Averaged Attention Maps')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images.shape,images[ImageNo].shape)\n",
    "input_img = np.expand_dims(images[ImageNo][0],2)\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(221)\n",
    "plt.imshow(originals[:,:,ImageNo*2],cmap='magma')\n",
    "plt.colorbar()\n",
    "plt.subplot(222)\n",
    "plt.imshow(originals[:,:,ImageNo*2+1],cmap='magma')\n",
    "plt.colorbar()\n",
    "plt.subplot(223)\n",
    "plt.imshow(attentions[:,:,ImageNo*2],cmap='magma')\n",
    "plt.colorbar()\n",
    "plt.subplot(224)\n",
    "plt.imshow(attentions[:,:,ImageNo*2+1],cmap='magma')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotNNFilterOverlay(input_img, \n",
    "                    np.log(np.expand_dims(attentions[:,:,ImageNo*2],2)),\n",
    "                    figure_id=2, interp='bilinear', \n",
    "                    colormap=cm.magma, title='Attention Overlay with Compatability 1', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotNNFilterOverlay(input_img, \n",
    "                    np.expand_dims(attentions[:,:,ImageNo*2+1],2),\n",
    "                    figure_id=2, interp='bilinear', \n",
    "                    colormap=cm.magma, title='Attention Overlay with Compatability 2', alpha=0.5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(attentions[:,:,0])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(attentions[:,:,1])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "print(len(attentions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting average of both attention maps:\n",
    "mean_attentions = np.expand_dims(np.mean(attentions[:,:,2*ImageNo:2*ImageNo+2],2),2)\n",
    "plotNNFilterOverlay(input_img, mean_attentions, figure_id=4, interp='bilinear', colormap='magma', title='Attention Overlay All Compatibilities', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotNNFilterOverlay(input_img, np.log(mean_attentions), figure_id=4, interp='bilinear', colormap='magma', title='Attention Overlay All Compatibilities (logged)', alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_attentions_temp = np.mean(attentions[:,:,2*ImageNo:2*ImageNo+2],2)\n",
    "mean_originals = (np.mean(originals[:,:,2*ImageNo:2*ImageNo+2],2))\n",
    "\n",
    "print(mean_attentions_temp.shape)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.imshow(np.log(originals[:,:,ImageNo*2]),cmap='magma')\n",
    "plt.title('Log of First Attention Map')\n",
    "plt.colorbar()\n",
    "\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.imshow(np.log(originals[:,:,ImageNo*2+1]),cmap='magma')\n",
    "plt.title('Log of Second Attention Map')\n",
    "plt.colorbar()\n",
    "\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.imshow(image4plot,cmap='gray')\n",
    "plt.contour(image4plot,3)\n",
    "plt.title('Source')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.imshow(np.log(mean_attentions_temp),cmap='magma')\n",
    "#plt.contour(mean_attentions[:,:,0],3)\n",
    "plt.title('Log of Averaged Attention Maps')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
